#!/usr/bin/env python3
"""
ë³‘ë ¬ ì§€ì‹ ë² ì´ìŠ¤ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
"""

import asyncio
import time
from src.core.models import WorkflowDefinition, WorkflowNode, WorkflowEdge
from src.core.node_execution_engine import NodeExecutionEngine

async def test_parallel_knowledge_base_search():
    """ì—¬ëŸ¬ ë…¸ë“œê°€ ë³‘ë ¬ë¡œ ì§€ì‹ ë² ì´ìŠ¤ë¥¼ ê²€ìƒ‰í•  ë•Œì˜ ë™ì‘ í…ŒìŠ¤íŠ¸"""
    
    # í…ŒìŠ¤íŠ¸ìš© ì›Œí¬í”Œë¡œìš° ìƒì„±: input -> 3ê°œì˜ generation ë…¸ë“œ (ë³‘ë ¬)
    nodes = [
        # Input node
        WorkflowNode(
            id="input-1",
            type="input-node",
            content="NVMe SSDì˜ ì„±ëŠ¥ ìš”êµ¬ì‚¬í•­ê³¼ ë³´ì•ˆ ìš”êµ¬ì‚¬í•­ì„ ë¶„ì„í•´ì£¼ì„¸ìš”."
        ),
        # 3ê°œì˜ Generation ë…¸ë“œ (ë³‘ë ¬ ì‹¤í–‰)
        WorkflowNode(
            id="gen-1", 
            type="generation-node",
            prompt="ì„±ëŠ¥ ê´€ë ¨ ìš”êµ¬ì‚¬í•­ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”: {input_data}\nì°¸ê³ : {context}",
            llm_provider="openai",
            model_type="gpt-4o-mini",
            knowledge_base="large_nvme_2-2",
            search_intensity="medium"
        ),
        WorkflowNode(
            id="gen-2",
            type="generation-node", 
            prompt="ë³´ì•ˆ ê´€ë ¨ ìš”êµ¬ì‚¬í•­ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”: {input_data}\nì°¸ê³ : {context}",
            llm_provider="openai",
            model_type="gpt-4o-mini",
            knowledge_base="large_nvme_2-2",
            search_intensity="medium"
        ),
        WorkflowNode(
            id="gen-3",
            type="generation-node",
            prompt="í˜¸í™˜ì„± ê´€ë ¨ ìš”êµ¬ì‚¬í•­ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”: {input_data}\nì°¸ê³ : {context}",
            llm_provider="openai", 
            model_type="gpt-4o-mini",
            knowledge_base="large_nvme_2-2",
            search_intensity="medium"
        )
    ]
    
    # ì—ì§€: input -> 3ê°œ generation ë…¸ë“œë“¤
    edges = [
        WorkflowEdge(source="input-1", target="gen-1"),
        WorkflowEdge(source="input-1", target="gen-2"), 
        WorkflowEdge(source="input-1", target="gen-3")
    ]
    
    workflow = WorkflowDefinition(nodes=nodes, edges=edges)
    
    print("=== ë³‘ë ¬ ì§€ì‹ ë² ì´ìŠ¤ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ===")
    print("3ê°œì˜ Generation ë…¸ë“œê°€ ë™ì‹œì— ê°™ì€ ì§€ì‹ ë² ì´ìŠ¤ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
    print("-" * 60)
    
    engine = NodeExecutionEngine()
    
    try:
        start_time = time.time()
        search_start_times = {}
        search_end_times = {}
        
        print("ğŸš€ ì›Œí¬í”Œë¡œìš° ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ ì‹œì‘\n")
        
        async for chunk in engine.execute_workflow_stream(workflow, False):
            if chunk.get("type") == "stream":
                content = chunk.get("content", "")
                node_id = chunk.get("node_id", "unknown")
                timestamp = time.time() - start_time
                
                print(f"[{timestamp:6.2f}s] {content}", end="")
                
                # ê²€ìƒ‰ ì‹œì‘/ì¢…ë£Œ ì‹œê°„ ì¶”ì 
                if "ê²€ìƒ‰ ì‹œì‘" in content:
                    search_start_times[node_id] = timestamp
                elif ("ë¬¸ì„œ ì°¾ìŒ" in content or "ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ" in content or "ê²€ìƒ‰ ì‹¤íŒ¨" in content):
                    search_end_times[node_id] = timestamp
                    
            elif chunk.get("type") == "complete":
                total_time = time.time() - start_time
                print(f"\nâœ… ì›Œí¬í”Œë¡œìš° ì™„ë£Œ (ì´ {total_time:.2f}ì´ˆ)\n")
                
                # ê²€ìƒ‰ ì‹œê°„ ë¶„ì„
                print("=== ê²€ìƒ‰ ì‹œê°„ ë¶„ì„ ===")
                for node_id in ["gen-1", "gen-2", "gen-3"]:
                    if node_id in search_start_times and node_id in search_end_times:
                        duration = search_end_times[node_id] - search_start_times[node_id]
                        print(f"{node_id}: {search_start_times[node_id]:.2f}s ~ {search_end_times[node_id]:.2f}s (ì†Œìš”: {duration:.2f}s)")
                
                # ë³‘ë ¬ì„± ë¶„ì„
                if len(search_start_times) >= 2:
                    start_times = list(search_start_times.values())
                    start_times.sort()
                    if (start_times[1] - start_times[0]) < 0.5:  # 0.5ì´ˆ ì´ë‚´ ì‹œì‘
                        print("\nâœ… ë³‘ë ¬ ê²€ìƒ‰ì´ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•˜ê³  ìˆìŠµë‹ˆë‹¤!")
                    else:
                        print(f"\nâš ï¸ ê²€ìƒ‰ì´ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰ë˜ê³  ìˆìŠµë‹ˆë‹¤ (ì‹œì°¨: {start_times[1] - start_times[0]:.2f}ì´ˆ)")
                
                break
                
            elif chunk.get("type") == "error":
                print(f"\nâŒ ì˜¤ë¥˜: {chunk.get('message')}")
                break
                
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(test_parallel_knowledge_base_search())