# API Keys
OPENAI_API_KEY=your_openai_api_key_here
GOOGLE_API_KEY=your_google_api_key_here
INTERNAL_API_KEY=your_internal_api_key_here

# Internal LLM Configuration (if using internal LLM)
INTERNAL_API_ENDPOINT=http://localhost:8000
INTERNAL_MODEL_NAME=your_model_name

# TEI (Text Embeddings Inference) Configuration
TEI_ENABLED=true                          # Enable/disable TEI (true/false)
TEI_BASE_URL=http://localhost:8080       # TEI server URL
TEI_TIMEOUT=30                           # Request timeout in seconds
TEI_TOKEN=                               # Optional: Authentication token for TEI server

# Note: When TEI_ENABLED=true, you must run TEI server with BAAI/bge-m3 model:
# docker run -p 8080:80 -v $pwd/data:/data --pull always ghcr.io/huggingface/text-embeddings-inference:latest --model-id BAAI/bge-m3

# When TEI_ENABLED=false, the system will use local sentence-transformers (all-MiniLM-L6-v2)
