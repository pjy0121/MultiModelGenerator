#!/usr/bin/env python3
"""
ì§€ì‹ ë² ì´ìŠ¤ ê²€ìƒ‰ ê³¼ì • ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸
"""

import asyncio
from src.core.models import WorkflowDefinition, WorkflowNode, WorkflowEdge
from src.api.node_executors import NodeExecutor

async def test_knowledge_base_streaming():
    """ì§€ì‹ ë² ì´ìŠ¤ ê²€ìƒ‰ ê³¼ì •ì´ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë³´ì´ëŠ”ì§€ í…ŒìŠ¤íŠ¸"""
    
    # í…ŒìŠ¤íŠ¸ìš© generation-node ìƒì„± (ì§€ì‹ ë² ì´ìŠ¤ í¬í•¨)
    generation_node = WorkflowNode(
        id="test-generation",
        type="generation-node",
        prompt="ë‹¤ìŒ ë‚´ìš©ì„ ë¶„ì„í•´ì„œ ìš”êµ¬ì‚¬í•­ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”.\n\nì…ë ¥: {input_data}\n\nì°¸ê³  ë¬¸ì„œ: {context}",
        llm_provider="openai",
        model_type="gpt-4o-mini",
        knowledge_base="large_nvme_2-2",  # ì¡´ì¬í•˜ëŠ” ì§€ì‹ ë² ì´ìŠ¤ ì‚¬ìš©
        search_intensity="medium"
    )
    
    # NodeExecutor ìƒì„±
    executor = NodeExecutor()
    
    print("=== ì§€ì‹ ë² ì´ìŠ¤ ê²€ìƒ‰ ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸ ===")
    print(f"Node ID: {generation_node.id}")
    print(f"Knowledge Base: {generation_node.knowledge_base}")
    print(f"Search Intensity: {generation_node.search_intensity}")
    print("-" * 60)
    
    try:
        # pre_outputsë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì œê³µ
        pre_outputs = ["NVMe SSDì˜ ì„±ëŠ¥ ìš”êµ¬ì‚¬í•­ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”."]
        
        print("ğŸš€ ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ ì‹œì‘:")
        print()
        
        # execute_node_streamìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
        chunk_count = 0
        knowledge_search_shown = False
        
        async for chunk in executor.execute_node_stream(generation_node, pre_outputs, False):
            chunk_count += 1
            
            if chunk.get("type") == "stream":
                content = chunk.get("content", "")
                print(f"[{chunk_count:03d}] {content}", end="")
                
                # ì§€ì‹ ë² ì´ìŠ¤ ê²€ìƒ‰ ê´€ë ¨ ë©”ì‹œì§€ê°€ ë‚˜ì™”ëŠ”ì§€ í™•ì¸
                if "ì§€ì‹ ë² ì´ìŠ¤" in content or "ê²€ìƒ‰" in content:
                    knowledge_search_shown = True
                    
            elif chunk.get("type") == "result":
                print(f"\n\n[ìµœì¢… ê²°ê³¼]")
                print(f"Success: {chunk.get('success')}")
                print(f"Output ê¸¸ì´: {len(chunk.get('output', ''))} ê¸€ì")
                
            elif chunk.get("type") == "parsed_result":
                print(f"\n\n[íŒŒì‹± ê²°ê³¼]")
                print(f"Success: {chunk.get('success')}")
                print(f"Output ê¸¸ì´: {len(chunk.get('output', ''))} ê¸€ì")
        
        print(f"\n\nâœ… ì´ {chunk_count}ê°œ ì²­í¬ ìˆ˜ì‹ ")
        
        if knowledge_search_shown:
            print("âœ… ì§€ì‹ ë² ì´ìŠ¤ ê²€ìƒ‰ ê³¼ì •ì´ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ í‘œì‹œë¨")
        else:
            print("âŒ ì§€ì‹ ë² ì´ìŠ¤ ê²€ìƒ‰ ê³¼ì •ì´ í‘œì‹œë˜ì§€ ì•ŠìŒ")
            
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(test_knowledge_base_streaming())