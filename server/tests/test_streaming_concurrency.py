"""
ìŠ¤íŠ¸ë¦¬ë° ë™ì‹œì„± í…ŒìŠ¤íŠ¸ 

ì—¬ëŸ¬ í´ë¼ì´ì–¸íŠ¸ê°€ ë™ì‹œì— ìŠ¤íŠ¸ë¦¬ë° ì›Œí¬í”Œë¡œìš°ë¥¼ ì‹¤í–‰í•  ë•Œ 
ì„œë¡œ ë¸”ë¡œí‚¹ë˜ì§€ ì•Šê³  ë…ë¦½ì ìœ¼ë¡œ ë™ì‘í•˜ëŠ”ì§€ ê²€ì¦
"""

import pytest
import asyncio
import aiohttp
import json
import time
from typing import Dict, Any, List, Tuple
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class TestStreamingConcurrency:
    """ìŠ¤íŠ¸ë¦¬ë° ë™ì‹œì„± í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    BASE_URL = "http://localhost:5001"
    
    @pytest.fixture
    def simple_workflow_data(self) -> Dict[str, Any]:
        """ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ìš© ì›Œí¬í”Œë¡œìš° ë°ì´í„° (LLM ì—†ëŠ” ë¹ ë¥¸ ë²„ì „)"""
        return {
            "workflow": {
                "nodes": [
                    {
                        "id": "input-1",
                        "type": "input-node",
                        "position": {"x": 100, "y": 100},
                        "content": "ìŠ¤íŠ¸ë¦¬ë° ë™ì‹œì„± í…ŒìŠ¤íŠ¸ìš© ì…ë ¥ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.",
                        "model_type": None,
                        "llm_provider": None,
                        "prompt": None,
                        "knowledge_base": None,
                        "search_intensity": None,
                        "rerank_provider": None,
                        "rerank_model": None,
                        "output": None,
                        "executed": False,
                        "error": None
                    },
                    {
                        "id": "output-1",
                        "type": "output-node",
                        "position": {"x": 300, "y": 100},
                        "content": "ìµœì¢… ê²°ê³¼",
                        "model_type": None,
                        "llm_provider": None,
                        "prompt": None,
                        "knowledge_base": None,
                        "search_intensity": None,
                        "rerank_provider": None,
                        "rerank_model": None,
                        "output": None,
                        "executed": False,
                        "error": None
                    }
                ],
                "edges": [
                    {"id": "edge-1", "source": "input-1", "target": "output-1"}
                ]
            }
        }

    async def _execute_streaming_client(
        self, 
        client_id: int, 
        session: aiohttp.ClientSession, 
        workflow_data: Dict[str, Any]
    ) -> Tuple[bool, float, int]:
        """ë‹¨ì¼ ìŠ¤íŠ¸ë¦¬ë° í´ë¼ì´ì–¸íŠ¸ ì‹¤í–‰"""
        start_time = time.time()
        logger.info(f"[Client {client_id}] ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘")
        
        try:
            async with session.post(
                f"{self.BASE_URL}/execute-workflow-stream",
                json=workflow_data,
                headers={"Content-Type": "application/json"}
            ) as response:
                
                if response.status != 200:
                    logger.error(f"[Client {client_id}] HTTP ì˜¤ë¥˜: {response.status}")
                    response_text = await response.text()
                    logger.error(f"[Client {client_id}] ì‘ë‹µ ë‚´ìš©: {response_text}")
                    return False, time.time() - start_time, 0
                
                # SSE ìŠ¤íŠ¸ë¦¼ ì½ê¸° - ë¼ì¸ ë‹¨ìœ„ë¡œ ë²„í¼ë§ ì²˜ë¦¬
                chunk_count = 0
                completed = False
                buffer = ""
                
                async for chunk in response.content.iter_chunked(1024):
                    buffer += chunk.decode('utf-8')
                    
                    # ë¼ì¸ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
                    lines = buffer.split('\n')
                    buffer = lines[-1]  # ë§ˆì§€ë§‰ ë¶ˆì™„ì „í•œ ë¼ì¸ì€ ë²„í¼ì— ìœ ì§€
                    
                    for line in lines[:-1]:
                        line = line.strip()
                        if line.startswith('data: '):
                            try:
                                data = json.loads(line[6:])
                                chunk_count += 1
                                
                                # ì£¼ìš” ì´ë²¤íŠ¸ ë¡œê·¸
                                if data.get('type') in ['start', 'node_start', 'complete', 'error']:
                                    logger.info(f"[Client {client_id}] {data.get('type')}: {data.get('message', '')}")
                                
                                # ì™„ë£Œ ë˜ëŠ” ì—ëŸ¬ ì‹œ ì¢…ë£Œ
                                if data.get('type') in ['complete', 'error']:
                                    completed = True
                                    elapsed = time.time() - start_time
                                    if data.get('type') == 'error':
                                        logger.error(f"[Client {client_id}] ì—ëŸ¬ ì‘ë‹µ: {data}")
                                    logger.info(f"[Client {client_id}] ì™„ë£Œ ({elapsed:.2f}ì´ˆ, {chunk_count}ê°œ ì²­í¬)")
                                    return data.get('type') == 'complete', elapsed, chunk_count
                                    
                            except json.JSONDecodeError as e:
                                logger.debug(f"[Client {client_id}] JSON íŒŒì‹± ì‹¤íŒ¨: {line[6:]} - {e}")
                                continue
                
                # ìŠ¤íŠ¸ë¦¼ì´ ì •ìƒ ì¢…ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸
                elapsed = time.time() - start_time
                if completed:
                    logger.info(f"[Client {client_id}] ìŠ¤íŠ¸ë¦¼ ì •ìƒ ì™„ë£Œ ({elapsed:.2f}ì´ˆ, {chunk_count}ê°œ ì²­í¬)")
                    return True, elapsed, chunk_count
                else:
                    logger.warning(f"[Client {client_id}] ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ without completion ({elapsed:.2f}ì´ˆ)")
                    return False, elapsed, chunk_count
                
        except Exception as e:
            elapsed = time.time() - start_time
            logger.error(f"[Client {client_id}] ì—ëŸ¬: {e} ({elapsed:.2f}ì´ˆ)")
            return False, elapsed, 0

    async def _call_api_endpoint(
        self, 
        session: aiohttp.ClientSession, 
        endpoint: str
    ) -> Tuple[bool, float]:
        """ë‹¨ì¼ API ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œ"""
        start_time = time.time()
        try:
            async with session.get(f"{self.BASE_URL}{endpoint}") as response:
                elapsed = time.time() - start_time
                success = response.status == 200
                if success:
                    await response.json()  # JSON ì‘ë‹µ íŒŒì‹±ê¹Œì§€ í™•ì¸
                return success, elapsed
        except Exception as e:
            elapsed = time.time() - start_time
            logger.error(f"API í˜¸ì¶œ ì‹¤íŒ¨ ({endpoint}): {e}")
            return False, elapsed

    @pytest.mark.asyncio
    async def test_concurrent_streaming_execution(self, simple_workflow_data):
        """ë™ì‹œ ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ í…ŒìŠ¤íŠ¸ - ë¸”ë¡œí‚¹ ì—†ì´ ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰ë˜ëŠ”ì§€ í™•ì¸"""
        logger.info("ğŸš€ ë™ì‹œ ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        num_clients = 3
        connector = aiohttp.TCPConnector(limit=100, limit_per_host=50)
        timeout = aiohttp.ClientTimeout(total=120)
        
        async with aiohttp.ClientSession(
            connector=connector, 
            timeout=timeout
        ) as session:
            
            # ë™ì‹œ ì‹¤í–‰
            tasks = [
                self._execute_streaming_client(i, session, simple_workflow_data)
                for i in range(1, num_clients + 1)
            ]
            
            start_time = time.time()
            results = await asyncio.gather(*tasks, return_exceptions=True)
            total_time = time.time() - start_time
            
            logger.info(f"ì „ì²´ í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {total_time:.2f}ì´ˆ")
            
            # ê²°ê³¼ ë¶„ì„
            successful = 0
            execution_times = []
            
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    logger.error(f"Client {i+1}: ì˜ˆì™¸ ë°œìƒ - {result}")
                    assert False, f"Client {i+1} failed with exception: {result}"
                else:
                    success, elapsed, chunk_count = result
                    if success:
                        successful += 1
                        execution_times.append(elapsed)
                        logger.info(f"Client {i+1}: ì„±ê³µ ({elapsed:.2f}ì´ˆ, {chunk_count}ê°œ ì²­í¬)")
                    else:
                        logger.error(f"Client {i+1}: ì‹¤íŒ¨ ({elapsed:.2f}ì´ˆ)")
            
            # ê²€ì¦
            assert successful == num_clients, f"ëª¨ë“  í´ë¼ì´ì–¸íŠ¸ê°€ ì„±ê³µí•´ì•¼ í•¨ (ì„±ê³µ: {successful}/{num_clients})"
            
            if execution_times:
                avg_time = sum(execution_times) / len(execution_times)
                min_time = min(execution_times)
                max_time = max(execution_times)
                time_variance = max_time - min_time
                
                logger.info(f"í‰ê·  ì‹¤í–‰ ì‹œê°„: {avg_time:.2f}ì´ˆ")
                logger.info(f"ìµœë‹¨/ìµœì¥ ì‹¤í–‰ ì‹œê°„: {min_time:.2f}ì´ˆ / {max_time:.2f}ì´ˆ")
                logger.info(f"ì‹œê°„ ì°¨ì´: {time_variance:.2f}ì´ˆ")
                
                # ë¸”ë¡œí‚¹ ì—¬ë¶€ íŒë‹¨ - ì‹¤í–‰ ì‹œê°„ ì°¨ì´ê°€ ë„ˆë¬´ í¬ë©´ ì•ˆë¨ (ê¸°ì¤€ ì™„í™”)
                assert time_variance < 20.0, f"í´ë¼ì´ì–¸íŠ¸ ê°„ ì‹¤í–‰ ì‹œê°„ ì°¨ì´ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤ ({time_variance:.2f}ì´ˆ). ë¸”ë¡œí‚¹ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                
                logger.info("âœ… ë™ì‹œ ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

    @pytest.mark.asyncio
    async def test_streaming_during_api_calls(self, simple_workflow_data):
        """ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ ì¤‘ ë‹¤ë¥¸ API í˜¸ì¶œì´ ë¸”ë¡œí‚¹ë˜ì§€ ì•ŠëŠ”ì§€ í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ”„ ìŠ¤íŠ¸ë¦¬ë° ì¤‘ API í˜¸ì¶œ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        connector = aiohttp.TCPConnector(limit=100, limit_per_host=50)
        timeout = aiohttp.ClientTimeout(total=120)
        
        async with aiohttp.ClientSession(
            connector=connector, 
            timeout=timeout
        ) as session:
            
            # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
            streaming_task = asyncio.create_task(
                self._execute_streaming_client(1, session, simple_workflow_data)
            )
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ í›„ ì ì‹œ ëŒ€ê¸°
            await asyncio.sleep(1.0)
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ ì¤‘ì— ë‹¤ë¥¸ APIë“¤ í˜¸ì¶œ
            api_endpoints = [
                "/",
                "/knowledge-bases", 
                "/available-models/google"
            ]
            
            api_tasks = [
                self._call_api_endpoint(session, endpoint)
                for endpoint in api_endpoints
            ]
            
            # API í˜¸ì¶œë“¤ì´ ë¹ ë¥´ê²Œ ì™„ë£Œë˜ëŠ”ì§€ í™•ì¸
            api_start_time = time.time()
            api_results = await asyncio.gather(*api_tasks, return_exceptions=True)
            api_total_time = time.time() - api_start_time
            
            # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ ëŒ€ê¸°
            streaming_result = await streaming_task
            
            # API ê²°ê³¼ ê²€ì¦
            logger.info(f"API í˜¸ì¶œë“¤ ì™„ë£Œ: {api_total_time:.3f}ì´ˆ")
            
            for i, (endpoint, result) in enumerate(zip(api_endpoints, api_results)):
                if isinstance(result, Exception):
                    assert False, f"API í˜¸ì¶œ ì‹¤íŒ¨ ({endpoint}): {result}"
                else:
                    success, elapsed = result
                    logger.info(f"{endpoint}: {elapsed:.3f}ì´ˆ ({'ì„±ê³µ' if success else 'ì‹¤íŒ¨'})")
                    assert success, f"API í˜¸ì¶œì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {endpoint}"
                    
                    # API í˜¸ì¶œì´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¬ë©´ ì•ˆë¨ (ë¸”ë¡œí‚¹ ì˜ì‹¬) - ê¸°ì¤€ ì™„í™”
                    assert elapsed < 10.0, f"API í˜¸ì¶œì´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤ ({endpoint}: {elapsed:.3f}ì´ˆ). ë¸”ë¡œí‚¹ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            
            # ìŠ¤íŠ¸ë¦¬ë° ê²°ê³¼ ê²€ì¦
            assert not isinstance(streaming_result, Exception), f"ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ ì‹¤íŒ¨: {streaming_result}"
            success, elapsed, chunk_count = streaming_result
            assert success, f"ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤ ({elapsed:.2f}ì´ˆ)"
            
            logger.info("âœ… ìŠ¤íŠ¸ë¦¬ë° ì¤‘ API í˜¸ì¶œì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

    @pytest.mark.asyncio
    async def test_api_response_times(self):
        """ê¸°ë³¸ API ì‘ë‹µ ì‹œê°„ í…ŒìŠ¤íŠ¸"""
        logger.info("â±ï¸ API ì‘ë‹µ ì‹œê°„ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        async with aiohttp.ClientSession() as session:
            api_endpoints = [
                ("/", "í—¬ìŠ¤ì²´í¬"),
                ("/knowledge-bases", "ì§€ì‹ë² ì´ìŠ¤ ëª©ë¡"),
                ("/available-models/google", "Google ëª¨ë¸ ëª©ë¡")
            ]
            
            for endpoint, description in api_endpoints:
                success, elapsed = await self._call_api_endpoint(session, endpoint)
                logger.info(f"{description}: {elapsed:.3f}ì´ˆ ({'ì„±ê³µ' if success else 'ì‹¤íŒ¨'})")
                
                assert success, f"{description} API í˜¸ì¶œ ì‹¤íŒ¨"
                # ê¸°ë³¸ APIë“¤ì€ 10ì´ˆ ì´ë‚´ì— ì‘ë‹µí•´ì•¼ í•¨ (ì´ˆê¸° ë¡œë“œ ë•…ë¬¸ì— ì‹œê°„ ì—¬ìœ )
                assert elapsed < 10.0, f"{description} API ì‘ë‹µì´ ë„ˆë¬´ ëŠë¦½ë‹ˆë‹¤ ({elapsed:.3f}ì´ˆ)"

    @pytest.mark.asyncio 
    async def test_concurrent_different_workflows(self, simple_workflow_data):
        """ì„œë¡œ ë‹¤ë¥¸ ì›Œí¬í”Œë¡œìš°ì˜ ë™ì‹œ ì‹¤í–‰ í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ”€ ì„œë¡œ ë‹¤ë¥¸ ì›Œí¬í”Œë¡œìš° ë™ì‹œ ì‹¤í–‰ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        # ë‘ ë²ˆì§¸ ì›Œí¬í”Œë¡œìš° ìƒì„± (ë” ê¸´ í”„ë¡¬í”„íŠ¸)
        workflow_2 = {
            "workflow": {
                "nodes": [
                    {
                        "id": "input-2",
                        "type": "input-node",
                        "position": {"x": 100, "y": 100},
                        "content": "ë‘ ë²ˆì§¸ ì›Œí¬í”Œë¡œìš°ì˜ í…ŒìŠ¤íŠ¸ ì…ë ¥ì…ë‹ˆë‹¤. ì´ í…ìŠ¤íŠ¸ëŠ” ë” ê¸¸ì–´ì„œ ì²˜ë¦¬ ì‹œê°„ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                        "model_type": None,
                        "llm_provider": None,
                        "prompt": None,
                        "knowledge_base": None,
                        "search_intensity": None,
                        "rerank_provider": None,
                        "rerank_model": None,
                        "output": None,
                        "executed": False,
                        "error": None
                    },
                    {
                        "id": "generation-2",
                        "type": "generation-node",
                        "position": {"x": 300, "y": 100},
                        "content": None,
                        "model_type": "gemini-2.0-flash",
                        "llm_provider": "google",
                        "prompt": "ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ìì„¸íˆ ë¶„ì„í•˜ê³  ì£¼ìš” í¬ì¸íŠ¸ 3ê°€ì§€ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”: {input_data}",
                        "knowledge_base": None,
                        "search_intensity": None,
                        "rerank_provider": None,
                        "rerank_model": None,
                        "output": None,
                        "executed": False,
                        "error": None
                    },
                    {
                        "id": "output-2",
                        "type": "output-node",
                        "position": {"x": 500, "y": 100},
                        "content": "ë¶„ì„ ê²°ê³¼",
                        "model_type": None,
                        "llm_provider": None,
                        "prompt": None,
                        "knowledge_base": None,
                        "search_intensity": None,
                        "rerank_provider": None,
                        "rerank_model": None,
                        "output": None,
                        "executed": False,
                        "error": None
                    }
                ],
                "edges": [
                    {"id": "edge-2-1", "source": "input-2", "target": "generation-2"},
                    {"id": "edge-2-2", "source": "generation-2", "target": "output-2"}
                ]
            }
        }
        
        connector = aiohttp.TCPConnector(limit=100, limit_per_host=50)
        timeout = aiohttp.ClientTimeout(total=120)
        
        async with aiohttp.ClientSession(
            connector=connector, 
            timeout=timeout
        ) as session:
            
            # ì„œë¡œ ë‹¤ë¥¸ ì›Œí¬í”Œë¡œìš° ë™ì‹œ ì‹¤í–‰
            tasks = [
                self._execute_streaming_client(1, session, simple_workflow_data),
                self._execute_streaming_client(2, session, workflow_2)
            ]
            
            start_time = time.time()
            results = await asyncio.gather(*tasks, return_exceptions=True)
            total_time = time.time() - start_time
            
            logger.info(f"ì„œë¡œ ë‹¤ë¥¸ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì™„ë£Œ: {total_time:.2f}ì´ˆ")
            
            # ê²°ê³¼ ê²€ì¦
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    assert False, f"Workflow {i+1} failed with exception: {result}"
                else:
                    success, elapsed, chunk_count = result
                    assert success, f"Workflow {i+1} ì‹¤í–‰ ì‹¤íŒ¨ ({elapsed:.2f}ì´ˆ)"
                    logger.info(f"Workflow {i+1}: ì„±ê³µ ({elapsed:.2f}ì´ˆ, {chunk_count}ê°œ ì²­í¬)")
            
            logger.info("âœ… ì„œë¡œ ë‹¤ë¥¸ ì›Œí¬í”Œë¡œìš° ë™ì‹œ ì‹¤í–‰ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

    @pytest.mark.asyncio
    async def test_concurrent_llm_workflows(self):
        """LLMì´ í¬í•¨ëœ ì›Œí¬í”Œë¡œìš° ë™ì‹œ ì‹¤í–‰ í…ŒìŠ¤íŠ¸ (Google API í‚¤ê°€ ìˆì„ ë•Œë§Œ)"""
        import os
        if not os.getenv("GOOGLE_API_KEY"):
            pytest.skip("Google API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‹¤ì œ LLM ë™ì‹œì„± í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        
        logger.info("ğŸ¤– LLM ì›Œí¬í”Œë¡œìš° ë™ì‹œì„± í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        # LLMì´ í¬í•¨ëœ ì›Œí¬í”Œë¡œìš°
        llm_workflow = {
            "workflow": {
                "nodes": [
                    {
                        "id": "input-llm",
                        "type": "input-node",
                        "position": {"x": 100, "y": 100},
                        "content": "ì´ê²ƒì€ LLM í…ŒìŠ¤íŠ¸ìš© ì§§ì€ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.",
                        "model_type": None,
                        "llm_provider": None,
                        "prompt": None,
                        "knowledge_base": None,
                        "search_intensity": None,
                        "rerank_provider": None,
                        "rerank_model": None,
                        "output": None,
                        "executed": False,
                        "error": None
                    },
                    {
                        "id": "generation-llm",
                        "type": "generation-node",
                        "position": {"x": 300, "y": 100},
                        "content": None,
                        "model_type": "gemini-1.5-flash",
                        "llm_provider": "google",
                        "prompt": "ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”: {input_data}",
                        "knowledge_base": None,
                        "search_intensity": None,
                        "rerank_provider": None,
                        "rerank_model": None,
                        "output": None,
                        "executed": False,
                        "error": None
                    },
                    {
                        "id": "output-llm",
                        "type": "output-node",
                        "position": {"x": 500, "y": 100},
                        "content": "LLM ê²°ê³¼",
                        "model_type": None,
                        "llm_provider": None,
                        "prompt": None,
                        "knowledge_base": None,
                        "search_intensity": None,
                        "rerank_provider": None,
                        "rerank_model": None,
                        "output": None,
                        "executed": False,
                        "error": None
                    }
                ],
                "edges": [
                    {"id": "edge-llm-1", "source": "input-llm", "target": "generation-llm"},
                    {"id": "edge-llm-2", "source": "generation-llm", "target": "output-llm"}
                ]
            }
        }
        
        connector = aiohttp.TCPConnector(limit=100, limit_per_host=50)
        timeout = aiohttp.ClientTimeout(total=180)  # LLM í˜¸ì¶œì„ ìœ„í•´ ë” ê¸´ íƒ€ì„ì•„ì›ƒ
        
        async with aiohttp.ClientSession(
            connector=connector, 
            timeout=timeout
        ) as session:
            
            # 2ê°œì˜ LLM ì›Œí¬í”Œë¡œìš° ë™ì‹œ ì‹¤í–‰
            tasks = [
                self._execute_streaming_client(1, session, llm_workflow),
                self._execute_streaming_client(2, session, llm_workflow)
            ]
            
            start_time = time.time()
            results = await asyncio.gather(*tasks, return_exceptions=True)
            total_time = time.time() - start_time
            
            logger.info(f"LLM ì›Œí¬í”Œë¡œìš° ë™ì‹œ ì‹¤í–‰ ì™„ë£Œ: {total_time:.2f}ì´ˆ")
            
            # ê²°ê³¼ ê²€ì¦
            successful = 0
            execution_times = []
            
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    logger.error(f"LLM Workflow {i+1}: ì˜ˆì™¸ ë°œìƒ - {result}")
                else:
                    success, elapsed, chunk_count = result
                    if success:
                        successful += 1
                        execution_times.append(elapsed)
                        logger.info(f"LLM Workflow {i+1}: ì„±ê³µ ({elapsed:.2f}ì´ˆ, {chunk_count}ê°œ ì²­í¬)")
                    else:
                        logger.error(f"LLM Workflow {i+1}: ì‹¤íŒ¨ ({elapsed:.2f}ì´ˆ)")
            
            # ìµœì†Œ 1ê°œëŠ” ì„±ê³µí•´ì•¼ í•¨ (API í‚¤ê°€ ìœ íš¨í•˜ë‹¤ë©´)
            assert successful >= 1, f"ìµœì†Œ 1ê°œì˜ LLM ì›Œí¬í”Œë¡œìš°ëŠ” ì„±ê³µí•´ì•¼ í•¨ (ì„±ê³µ: {successful}/2)"
            
            if len(execution_times) >= 2:
                time_variance = max(execution_times) - min(execution_times)
                logger.info(f"LLM ì‹¤í–‰ ì‹œê°„ ì°¨ì´: {time_variance:.2f}ì´ˆ")
                
                # LLM í˜¸ì¶œì€ ì‹œê°„ì´ ë” ê±¸ë¦´ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê´€ëŒ€í•œ ê¸°ì¤€
                assert time_variance < 30.0, f"LLM ì›Œí¬í”Œë¡œìš° ê°„ ì‹¤í–‰ ì‹œê°„ ì°¨ì´ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤ ({time_variance:.2f}ì´ˆ)"
            
            logger.info("âœ… LLM ì›Œí¬í”Œë¡œìš° ë™ì‹œì„± í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    # ë‹¨ë… ì‹¤í–‰ ì‹œ (ê°œë°œ/ë””ë²„ê¹…ìš©)
    import sys
    import os
    
    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
    sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))
    
    pytest.main([__file__, "-v", "-s"])