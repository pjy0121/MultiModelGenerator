import time
from typing import List, Optional, Dict
from ..core.models import NodeConfig, NodeOutput
from ..services.llm_factory import LLMFactory

class LayerEngine:
    """Layerë³„ ë…¸ë“œ ì‹¤í–‰ ì—”ì§„ - ì¶”ìƒí™”ëœ LLM í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©"""
    
    def __init__(self):
        """LayerEngine ì´ˆê¸°í™” - ì§€ì—° ë¡œë”©ìœ¼ë¡œ ì‹¤ì œ ì‚¬ìš© ì‹œ LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        self._initialized = False
        self._available_providers = None
    
    def _ensure_initialized(self):
        """LLM íŒ©í† ë¦¬ ì´ˆê¸°í™” í™•ì¸ (ì§€ì—° ë¡œë”©)"""
        if self._initialized:
            return
            
        try:
            # LLM íŒ©í† ë¦¬ë¥¼ í†µí•´ ì‚¬ìš© ê°€ëŠ¥í•œ ì œê³µì í™•ì¸
            self._available_providers = LLMFactory.get_available_providers()
            if not self._available_providers:
                raise RuntimeError("ì‚¬ìš© ê°€ëŠ¥í•œ LLM ì œê³µìê°€ ì—†ìŠµë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            
            print(f"âœ… ì‚¬ìš© ê°€ëŠ¥í•œ LLM ì œê³µì: {', '.join(self._available_providers)}")
            self._initialized = True
            
        except Exception as e:
            print(f"âš ï¸ LayerEngine ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            print("ğŸ’¡ .env íŒŒì¼ì˜ API í‚¤ë“¤ì´ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
            raise
    
    def execute_node(self, node: NodeConfig, input_data: str, context_chunks: List[str]) -> NodeOutput:
        """ê°œë³„ ë…¸ë“œ ì‹¤í–‰ - LLM íŒ©í† ë¦¬ë¥¼ í†µí•œ ë™ì  í´ë¼ì´ì–¸íŠ¸ ì„ íƒ"""
        start_time = time.time()
        
        try:
            # ì§€ì—° ë¡œë”©ìœ¼ë¡œ ì´ˆê¸°í™” í™•ì¸
            self._ensure_initialized()
            
            # ëª¨ë¸ íƒ€ì… ê²°ì • (ì¸ë±ìŠ¤ ìš°ì„ , í˜¸í™˜ì„± ì§€ì›)
            model_name = self._resolve_model_name(node)
            if not model_name:
                raise ValueError("ìœ íš¨í•œ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # ì„ íƒëœ ëª¨ë¸ì— ë§ëŠ” í´ë¼ì´ì–¸íŠ¸ ìë™ ì„ íƒ
            try:
                # provider ì •ë³´ê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ìë™ ì„ íƒ
                provider = getattr(node, 'provider', None)
                llm_client = LLMFactory.get_client_for_model(model_name, provider)
                print(f"âœ… ëª¨ë¸ {model_name}ì— ëŒ€í•´ {llm_client.__class__.__name__} ì‚¬ìš©")
            except Exception as e:
                # ì‚¬ìš© ë¶ˆê°€ëŠ¥í•œ ëª¨ë¸ì¸ ê²½ìš° ëª…í™•í•œ ì˜¤ë¥˜ ë©”ì‹œì§€
                raise ValueError(f"ëª¨ë¸ '{model_name}'ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}")
            
            # LLM API í˜¸ì¶œ
            result = self._execute_llm_node(llm_client, node, input_data, context_chunks, model_name)
            
            execution_time = time.time() - start_time
            
            return NodeOutput(
                node_id=node.id,
                model_type=model_name,
                requirements=result,
                execution_time=execution_time
            )
            
        except Exception as e:
            execution_time = time.time() - start_time
            return NodeOutput(
                node_id=node.id,
                model_type="unknown",
                requirements=f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                execution_time=execution_time
            )
    
    def _resolve_model_name(self, node: NodeConfig) -> Optional[str]:
        """ë…¸ë“œì˜ ëª¨ë¸ëª… ê²°ì •"""
        if node.model:
            print(f"âœ… ë…¸ë“œì—ì„œ ëª¨ë¸ ì‚¬ìš©: {node.model}")
            return node.model
        
        print("âŒ ëª¨ë¸ì„ ê²°ì •í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. model í•„ë“œê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        return None
    
    def _execute_llm_node(self, llm_client, node: NodeConfig, input_data: str, context_chunks: List[Dict], model_name: str) -> str:
        """LLM ë…¸ë“œ ì‹¤í–‰ - í´ë¼ì´ì–¸íŠ¸ì—ì„œ ì™„ì„±ëœ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©"""
        # í´ë¼ì´ì–¸íŠ¸ì—ì„œ ì´ë¯¸ {context}ì™€ {layer_input}ì„ ëª¨ë‘ ì²˜ë¦¬í•œ ì™„ì„±ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ë°›ìŒ
        formatted_prompt = node.prompt
        
        print(f"ğŸ¤– ì‚¬ìš©í•  ëª¨ë¸: {model_name}")
        if context_chunks:
            print(f"ğŸ“š ì „ë‹¬ë°›ì€ context_chunks: {len(context_chunks)}ê°œ")
            for i, chunk in enumerate(context_chunks[:2]):  # ì²˜ìŒ 2ê°œ ì²­í¬ë§Œ ë¡œê¹…
                chunk_text = str(chunk) if not isinstance(chunk, str) else chunk
                print(f"ğŸ“„ ì²­í¬ {i+1}: {chunk_text[:100]}...")
        else:
            print("âš ï¸ context_chunksê°€ ë¹„ì–´ìˆìŒ")

        # ê°„ë‹¨í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        system_prompt = "ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ìš”ì²­ì— ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."
        
        try:
            # LLM í´ë¼ì´ì–¸íŠ¸ ì¸í„°í˜ì´ìŠ¤ë¥¼ í†µí•œ í˜¸ì¶œ
            result = llm_client.chat_completion(
                model=model_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": formatted_prompt}
                ],
                temperature=0.1,
                max_tokens=2500
            )
            
            return result
            
        except Exception as e:
            error_msg = str(e)
            print(f"âŒ LLM API í˜¸ì¶œ ì˜¤ë¥˜: {error_msg}")
            return f"âš ï¸ API í˜¸ì¶œ ì˜¤ë¥˜: {error_msg}"
