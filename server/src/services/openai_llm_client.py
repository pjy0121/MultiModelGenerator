import asyncio
from typing import List, Dict, Any
from openai import OpenAI
from .llm_client_interface import LLMClientInterface
from ..config import API_KEYS, NODE_EXECUTION_CONFIG

class OpenAIClient(LLMClientInterface):
    """OpenAI API í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self):
        """OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        self.client = None
        self._initialize_client()
    
    def _initialize_client(self):
        """í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        try:
            if not API_KEYS["openai"]:
                raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                
            self.client = OpenAI(
                api_key=API_KEYS["openai"]
            )
        except Exception as e:
            self.client = None
    
    def is_available(self) -> bool:
        """OpenAI API ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        return self.client is not None and bool(API_KEYS["openai"])

    def get_available_models(self) -> List[Dict[str, Any]]:
        """OpenAI APIì—ì„œ ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        if not self.is_available():
            return []
        
        try:
            models = self.client.models.list()
            available_models = []
            
            # GPT ëª¨ë¸ë“¤ë§Œ í•„í„°ë§ (ì±„íŒ… ì™„ì„±ìš© ëª¨ë¸ë“¤)
            chat_model_prefixes = ["gpt-3.5", "gpt-4"]
            
            for model in models.data:
                model_id = model.id
                # ì±„íŒ… ì™„ì„±ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ë“¤ë§Œ í¬í•¨
                if any(model_id.startswith(prefix) for prefix in chat_model_prefixes):
                    model_info = {
                        "value": model_id,
                        "label": model_id,
                        "provider": "openai",
                        "model_type": model_id,
                        "disabled": False
                    }
                    available_models.append(model_info)
                
            return available_models
            
        except Exception as e:
            return []
    
    async def generate_stream(
        self, 
        prompt: str, 
        model: str, 
        temperature: float = 0.3, 
        max_tokens: int = NODE_EXECUTION_CONFIG["max_tokens_default"]
    ):
        """ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì‘ë‹µ ìƒì„± (í†µí•©ëœ ë‹¨ì¼ ì¸í„°í˜ì´ìŠ¤)"""
        if not self.client:
            raise RuntimeError("OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        try:
            messages = [{"role": "user", "content": prompt}]
            stream = self.client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                stream=True
            )
            
            for chunk in stream:
                content = chunk.choices[0].delta.content
                if content is not None:
                    yield content
                    await asyncio.sleep(0.01)
                    
        except Exception as e:
            # OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ì •í™•í•œ ì—ëŸ¬ íƒ€ì… ì²˜ë¦¬
            from openai import AuthenticationError, APIError, RateLimitError
            
            if isinstance(e, AuthenticationError):
                error_msg = (
                    f"OpenAI API ì¸ì¦ ì‹¤íŒ¨: {e}\n"
                    f"ğŸ’¡ í•´ê²° ë°©ë²•:\n"
                    f"1. .env íŒŒì¼ì˜ OPENAI_API_KEY í™•ì¸\n"
                    f"2. API í‚¤ê°€ ìœ íš¨í•œì§€ í™•ì¸\n"
                    f"3. ì„œë²„ ì¬ì‹œì‘ í›„ ë‹¤ì‹œ ì‹œë„"
                )
                raise RuntimeError(error_msg)
            elif isinstance(e, RateLimitError):
                raise RuntimeError(f"OpenAI API ì‚¬ìš©ëŸ‰ ì´ˆê³¼: {e}")
            elif isinstance(e, APIError):
                raise RuntimeError(f"OpenAI API ì˜¤ë¥˜: {e}")
            else:
                raise RuntimeError(f"OpenAI API ìŠ¤íŠ¸ë¦¬ë° ìš”ì²­ ì‹¤íŒ¨: {e}")
