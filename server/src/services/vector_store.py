import chromadb
import os
from typing import List, Dict
from ..core.config import Config
from .rerank import ReRanker
from .llm_factory import LLMFactory

class VectorStore:
    def __init__(self, kb_name: str):
        self.kb_name = kb_name
        self.db_path = Config.get_kb_path(kb_name)
        
        # ì§€ì‹ ë² ì´ìŠ¤ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.db_path, exist_ok=True)
        
        self.client = chromadb.PersistentClient(path=self.db_path)
        self.collection = self.client.get_or_create_collection(
            name="spec_documents",
            metadata={"hnsw:space": "cosine"}
        )
    
    def store_chunks(self, chunks: List[Dict]) -> None:
        """ì²­í¬ë“¤ì„ ë²¡í„° DBì— ì €ì¥"""
        print(f"ğŸ’¾ ì§€ì‹ ë² ì´ìŠ¤ '{self.kb_name}'ì— {len(chunks)}ê°œ ì²­í¬ ì €ì¥ ì¤‘...")
        
        # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (ìƒˆë¡œ ì €ì¥í•˜ëŠ” ê²½ìš°)
        try:
            self.collection.delete()
            self.collection = self.client.get_or_create_collection(
                name="spec_documents",
                metadata={"hnsw:space": "cosine"}
            )
        except:
            pass
        
        ids = [f"chunk_{chunk['id']}" for chunk in chunks]
        documents = [chunk['content'] for chunk in chunks]
        embeddings = [chunk['embedding'] for chunk in chunks]
        metadatas = [{'length': chunk['length'], 'chunk_id': chunk['id']} for chunk in chunks]
        
        # ë°°ì¹˜ í¬ê¸°ë¡œ ë‚˜ëˆ„ì–´ ì €ì¥ (ChromaDB ì œí•œ)
        batch_size = 100
        for i in range(0, len(chunks), batch_size):
            end_idx = min(i + batch_size, len(chunks))
            
            self.collection.add(
                ids=ids[i:end_idx],
                documents=documents[i:end_idx],
                embeddings=embeddings[i:end_idx],
                metadatas=metadatas[i:end_idx]
            )
        
        print(f"âœ… ì§€ì‹ ë² ì´ìŠ¤ '{self.kb_name}' ì €ì¥ ì™„ë£Œ!")

    async def _search_initial_chunks(self, query: str, top_k: int) -> List[str]:
        """ì´ˆê¸° ë²¡í„° ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ëŠ” ë‚´ë¶€ í—¬í¼ í•¨ìˆ˜"""
        print(f"ğŸ” ì§€ì‹ ë² ì´ìŠ¤ '{self.kb_name}'ì—ì„œ í‚¤ì›Œë“œ '{query}' ì´ˆê¸° ê²€ìƒ‰ ì¤‘... (top_k={top_k})")
        
        try:
            collection_count = self.collection.count()
            if collection_count == 0:
                print("âŒ ì§€ì‹ ë² ì´ìŠ¤ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                return []
            
            actual_top_k = min(top_k, collection_count)
            
            results = self.collection.query(
                query_texts=[query],
                n_results=actual_top_k,
                include=['documents', 'distances']
            )
            
            if not results['documents'] or not results['documents'][0]:
                print("âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                return []

            initial_chunks = results['documents'][0]
            distances = results['distances'][0] if results['distances'] else []
            
            filtered_chunks = [
                chunk for chunk, distance in zip(initial_chunks, distances)
                if distance <= Config.SEARCH_SIMILARITY_THRESHOLD
            ]
            
            print(f"ğŸ“š 1ì°¨ í•„í„°ë§ í›„ {len(filtered_chunks)}ê°œ ê´€ë ¨ ì²­í¬ ë°œê²¬.")
            return filtered_chunks

        except Exception as e:
            print(f"âš ï¸ ì´ˆê¸° ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return []

    async def search_with_rerank(
        self, 
        query: str, 
        search_intensity: str,
        rerank_provider: str,
        rerank_model: str
    ) -> List[str]:
        """ìœ ì‚¬í•œ ì²­í¬ ê²€ìƒ‰ í›„ LLMìœ¼ë¡œ ì¬ì •ë ¬í•©ë‹ˆë‹¤."""
        search_params = Config.SEARCH_INTENSITY_MAP.get(search_intensity, Config.SEARCH_INTENSITY_MAP["medium"])
        top_k_init = search_params["init"]
        top_k_final = search_params["final"]

        initial_chunks = await self._search_initial_chunks(query, top_k_init)
        if not initial_chunks:
            return []

        try:
            reranker = ReRanker(provider=rerank_provider, model=rerank_model)
            reranked_chunks = await reranker.rerank_documents(query, initial_chunks, top_k_final)
            return reranked_chunks
        except Exception as e:
            print(f"âš ï¸ ì¬ì •ë ¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. ì´ˆê¸° ê²€ìƒ‰ ê²°ê³¼ì˜ ì¼ë¶€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.")
            return initial_chunks[:top_k_final]

    async def search_without_rerank(self, query: str, search_intensity: str) -> List[str]:
        """ìœ ì‚¬í•œ ì²­í¬ë¥¼ ê²€ìƒ‰ë§Œ í•˜ê³  ì¬ì •ë ¬ì€ ìˆ˜í–‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."""
        search_params = Config.SEARCH_INTENSITY_MAP.get(search_intensity, Config.SEARCH_INTENSITY_MAP["medium"])
        top_k = search_params["final"]

        # ì—¬ê¸°ì„œëŠ” ì´ˆê¸° ê²€ìƒ‰ì˜ top_kë¥¼ ìµœì¢… ê²°ê³¼ ê°œìˆ˜ì™€ ë™ì¼í•˜ê²Œ ì„¤ì •
        initial_chunks = await self._search_initial_chunks(query, top_k * 2) # ì—¬ìœ ìˆê²Œ 2ë°°ìˆ˜ ê²€ìƒ‰
        return initial_chunks[:top_k]
    
    def get_status(self) -> dict:
        """ì§€ì‹ ë² ì´ìŠ¤ ìƒíƒœ ì •ë³´ ë°˜í™˜"""
        try:
            count = self.collection.count()
            return {
                'exists': True,
                'count': count,
                'path': self.db_path,
                'name': self.kb_name
            }
        except:
            return {
                'exists': False,
                'count': 0,
                'path': self.db_path,
                'name': self.kb_name
            }


class VectorStoreService:
    """ë…¸ë“œ ê¸°ë°˜ ì›Œí¬í”Œë¡œìš°ë¥¼ ìœ„í•œ ë²¡í„° ìŠ¤í† ì–´ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        """VectorStoreService ì´ˆê¸°í™”"""
        self.vector_stores: Dict[str, VectorStore] = {}

    def get_vector_store(self, kb_name: str) -> VectorStore:
        """ì§€ì •ëœ ì§€ì‹ ë² ì´ìŠ¤ì— ëŒ€í•œ VectorStore ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤ (ìºì‹± ì‚¬ìš©)"""
        if kb_name not in self.vector_stores:
            self.vector_stores[kb_name] = VectorStore(kb_name)
        return self.vector_stores[kb_name]
    
    def get_knowledge_bases(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ì§€ì‹ ë² ì´ìŠ¤ ëª©ë¡ ë°˜í™˜"""
        try:
            kb_base_path = os.path.join(os.path.dirname(__file__), '..', '..', 'knowledge_bases')
            if not os.path.exists(kb_base_path):
                return []
            
            knowledge_bases = []
            for item in os.listdir(kb_base_path):
                item_path = os.path.join(kb_base_path, item)
                if os.path.isdir(item_path):
                    knowledge_bases.append(item)
            
            return knowledge_bases
            
        except Exception as e:
            print(f"ì§€ì‹ ë² ì´ìŠ¤ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def list_knowledge_bases(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ì§€ì‹ ë² ì´ìŠ¤ ëª©ë¡ ë°˜í™˜ (alias for get_knowledge_bases)"""
        return self.get_knowledge_bases()
    
    def get_collection(self, kb_name: str):
        """ì§€ì •ëœ ì§€ì‹ ë² ì´ìŠ¤ì˜ ì»¬ë ‰ì…˜ ë°˜í™˜"""
        try:
            vector_store = self.get_vector_store(kb_name)
            return vector_store.collection
        except Exception as e:
            print(f"ì»¬ë ‰ì…˜ ì¡°íšŒ ì‹¤íŒ¨ ({kb_name}): {e}")
            return None
    
    def get_knowledge_base_info(self, kb_name: str) -> Dict:
        """ì§€ì •ëœ ì§€ì‹ ë² ì´ìŠ¤ì˜ ìƒì„¸ ì •ë³´ ë°˜í™˜"""
        try:
            vector_store = self.get_vector_store(kb_name)
            status = vector_store.get_status()
            return {
                'name': kb_name,
                'exists': status.get('exists', False),
                'chunk_count': status.get('count', 0),
                'path': status.get('path', ''),
                'created_at': 'Unknown'
            }
        except Exception as e:
            print(f"ì§€ì‹ ë² ì´ìŠ¤ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨ ({kb_name}): {e}")
            return {
                'name': kb_name,
                'exists': False,
                'chunk_count': 0,
                'path': '',
                'created_at': 'Unknown'
            }
