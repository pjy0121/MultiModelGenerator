import chromadb
import os
from typing import List, Dict
from ..core.config import Config

class VectorStore:
    def __init__(self, kb_name: str):
        self.kb_name = kb_name
        self.db_path = Config.get_kb_path(kb_name)
        
        # ì§€ì‹ ë² ì´ìŠ¤ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.db_path, exist_ok=True)
        
        self.client = chromadb.PersistentClient(path=self.db_path)
        self.collection = self.client.get_or_create_collection(
            name="spec_documents",
            metadata={"hnsw:space": "cosine"}
        )
    
    def store_chunks(self, chunks: List[Dict]) -> None:
        """ì²­í¬ë“¤ì„ ë²¡í„° DBì— ì €ì¥"""
        print(f"ğŸ’¾ ì§€ì‹ ë² ì´ìŠ¤ '{self.kb_name}'ì— {len(chunks)}ê°œ ì²­í¬ ì €ì¥ ì¤‘...")
        
        # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (ìƒˆë¡œ ì €ì¥í•˜ëŠ” ê²½ìš°)
        try:
            self.collection.delete()
            self.collection = self.client.get_or_create_collection(
                name="spec_documents",
                metadata={"hnsw:space": "cosine"}
            )
        except:
            pass
        
        ids = [f"chunk_{chunk['id']}" for chunk in chunks]
        documents = [chunk['content'] for chunk in chunks]
        embeddings = [chunk['embedding'] for chunk in chunks]
        metadatas = [{'length': chunk['length'], 'chunk_id': chunk['id']} for chunk in chunks]
        
        # ë°°ì¹˜ í¬ê¸°ë¡œ ë‚˜ëˆ„ì–´ ì €ì¥ (ChromaDB ì œí•œ)
        batch_size = 100
        for i in range(0, len(chunks), batch_size):
            end_idx = min(i + batch_size, len(chunks))
            
            self.collection.add(
                ids=ids[i:end_idx],
                documents=documents[i:end_idx],
                embeddings=embeddings[i:end_idx],
                metadatas=metadatas[i:end_idx]
            )
        
        print(f"âœ… ì§€ì‹ ë² ì´ìŠ¤ '{self.kb_name}' ì €ì¥ ì™„ë£Œ!")
    
    def search_similar_chunks(self, query: str, top_k: int = None) -> List[str]:
        """ìœ ì‚¬í•œ ì²­í¬ ê²€ìƒ‰"""
        if top_k is None:
            top_k = Config.SEARCH_TOP_K
        
        # ìµœëŒ€ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ ì œí•œ ì ìš©
        if top_k > Config.SEARCH_MAX_TOP_K:
            top_k = Config.SEARCH_MAX_TOP_K
            
        print(f"ğŸ” ì§€ì‹ ë² ì´ìŠ¤ '{self.kb_name}'ì—ì„œ í‚¤ì›Œë“œ '{query}' ê²€ìƒ‰ ì¤‘... (top_k={top_k})")
        
        try:
            # ì»¬ë ‰ì…˜ì´ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
            collection_count = self.collection.count()
            if collection_count == 0:
                print("âŒ ì§€ì‹ ë² ì´ìŠ¤ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                return []
            
            # í¬ê´„ì  ê²€ìƒ‰ ëª¨ë“œê°€ í™œì„±í™”ëœ ê²½ìš°, ë” ë§ì€ ê²°ê³¼ ê²€ìƒ‰
            if Config.SEARCH_ENABLE_COMPREHENSIVE:
                # ì „ì²´ ë¬¸ì„œì˜ 80% ë˜ëŠ” ì„¤ì •ëœ top_k ì¤‘ ë” í° ê°’ ì‚¬ìš©
                comprehensive_top_k = max(top_k, int(collection_count * 0.8))
                actual_top_k = min(comprehensive_top_k, collection_count)
                print(f"ğŸ“– í¬ê´„ì  ê²€ìƒ‰ ëª¨ë“œ: {actual_top_k}ê°œ ì²­í¬ ê²€ìƒ‰ (ì „ì²´ {collection_count}ê°œ ì¤‘)")
            else:
                # ì‹¤ì œ ê²€ìƒ‰í•  ê²°ê³¼ ìˆ˜ ì¡°ì • (ì „ì²´ ì²­í¬ ìˆ˜ë³´ë‹¤ ë§ì„ ìˆ˜ ì—†ìŒ)
                actual_top_k = min(top_k, collection_count)
            
            results = self.collection.query(
                query_texts=[query],
                n_results=actual_top_k,
                include=['documents', 'distances', 'metadatas']
            )
            
            if results['documents'] and results['documents'][0]:
                chunks = results['documents'][0]
                distances = results['distances'][0] if results['distances'] else []
                
                # ìœ ì‚¬ë„ ì ìˆ˜ ê¸°ë°˜ í•„í„°ë§ (ì„¤ì •ëœ ì„ê³„ê°’ ì‚¬ìš©)
                filtered_chunks = []
                for i, (chunk, distance) in enumerate(zip(chunks, distances)):
                    if distance <= Config.SEARCH_SIMILARITY_THRESHOLD:
                        filtered_chunks.append(chunk)
                    # í¬ê´„ì  ê²€ìƒ‰ ëª¨ë“œì—ì„œëŠ” ì¤‘ë‹¨í•˜ì§€ ì•Šê³  ëª¨ë“  ê²°ê³¼ í™•ì¸
                    elif not Config.SEARCH_ENABLE_COMPREHENSIVE:
                        break  # ì¼ë°˜ ëª¨ë“œì—ì„œë§Œ ì¤‘ë‹¨
                
                print(f"ğŸ“š {len(filtered_chunks)}ê°œ ê´€ë ¨ ì²­í¬ ë°œê²¬ (ì´ {len(chunks)}ê°œ ê²€ìƒ‰ ê²°ê³¼ ì¤‘)")
                return filtered_chunks
            else:
                print("âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                return []
                
        except Exception as e:
            print(f"âš ï¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return []
    
    def get_status(self) -> dict:
        """ì§€ì‹ ë² ì´ìŠ¤ ìƒíƒœ ì •ë³´ ë°˜í™˜"""
        try:
            count = self.collection.count()
            return {
                'exists': True,
                'count': count,
                'path': self.db_path,
                'name': self.kb_name
            }
        except:
            return {
                'exists': False,
                'count': 0,
                'path': self.db_path,
                'name': self.kb_name
            }


class VectorStoreService:
    """ë…¸ë“œ ê¸°ë°˜ ì›Œí¬í”Œë¡œìš°ë¥¼ ìœ„í•œ ë²¡í„° ìŠ¤í† ì–´ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        """VectorStoreService ì´ˆê¸°í™”"""
        pass
    
    def search(self, query: str, collection_name: str, top_k: int = 5) -> List[Dict]:
        """
        ì§€ì •ëœ ì»¬ë ‰ì…˜ì—ì„œ ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            collection_name: ì§€ì‹ ë² ì´ìŠ¤ ì´ë¦„
            top_k: ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ [{"content": "...", "distance": 0.5}, ...]
        """
        try:
            vector_store = VectorStore(collection_name)
            chunks = vector_store.search_similar_chunks(query, top_k)
            
            # VectorStore ê²°ê³¼ë¥¼ í†µì¼ëœ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            results = []
            for chunk in chunks:
                results.append({
                    "content": chunk,
                    "distance": 0.0  # VectorStoreëŠ” ê±°ë¦¬ ì •ë³´ë¥¼ ë°˜í™˜í•˜ì§€ ì•ŠìŒ
                })
            
            return results
            
        except Exception as e:
            print(f"ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def get_knowledge_bases(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ì§€ì‹ ë² ì´ìŠ¤ ëª©ë¡ ë°˜í™˜"""
        try:
            kb_base_path = os.path.join(os.path.dirname(__file__), '..', '..', 'knowledge_bases')
            if not os.path.exists(kb_base_path):
                return []
            
            knowledge_bases = []
            for item in os.listdir(kb_base_path):
                item_path = os.path.join(kb_base_path, item)
                if os.path.isdir(item_path):
                    knowledge_bases.append(item)
            
            return knowledge_bases
            
        except Exception as e:
            print(f"ì§€ì‹ ë² ì´ìŠ¤ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def list_knowledge_bases(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ì§€ì‹ ë² ì´ìŠ¤ ëª©ë¡ ë°˜í™˜ (alias for get_knowledge_bases)"""
        return self.get_knowledge_bases()
    
    def get_collection(self, kb_name: str):
        """ì§€ì •ëœ ì§€ì‹ ë² ì´ìŠ¤ì˜ ì»¬ë ‰ì…˜ ë°˜í™˜"""
        try:
            vector_store = VectorStore(kb_name)
            return vector_store.collection
        except Exception as e:
            print(f"ì»¬ë ‰ì…˜ ì¡°íšŒ ì‹¤íŒ¨ ({kb_name}): {e}")
            return None
    
    def get_knowledge_base_info(self, kb_name: str) -> Dict:
        """ì§€ì •ëœ ì§€ì‹ ë² ì´ìŠ¤ì˜ ìƒì„¸ ì •ë³´ ë°˜í™˜"""
        try:
            vector_store = VectorStore(kb_name)
            status = vector_store.get_status()
            return {
                'name': kb_name,
                'exists': status.get('exists', False),
                'chunk_count': status.get('count', 0),  # document_count -> chunk_count
                'path': status.get('path', ''),
                'created_at': 'Unknown'
            }
        except Exception as e:
            print(f"ì§€ì‹ ë² ì´ìŠ¤ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨ ({kb_name}): {e}")
            return {
                'name': kb_name,
                'exists': False,
                'chunk_count': 0,  # document_count -> chunk_count
                'path': '',
                'created_at': 'Unknown'
            }
