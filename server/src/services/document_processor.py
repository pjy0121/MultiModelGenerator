import PyPDF2
import re
from typing import List, Dict
from ..core.config import VECTOR_DB_CONFIG
from transformers import AutoTokenizer

# TEI ë˜ëŠ” ë¡œì»¬ ëª¨ë¸ ì¡°ê±´ë¶€ import
config = VECTOR_DB_CONFIG
if config.get('tei_enabled', False):
    from .tei_embedding import TEIClient
else:
    from sentence_transformers import SentenceTransformer

class DocumentProcessor:
    def __init__(self, chunk_size: int = None, chunk_overlap: int = None):
        self.chunk_size = chunk_size or VECTOR_DB_CONFIG["chunk_size"]
        self.chunk_overlap = chunk_overlap or VECTOR_DB_CONFIG["chunk_overlap"]
        
        # BGE-M3 tokenizer ì´ˆê¸°í™” (token ê¸°ë°˜ ì²­í‚¹ìš©)
        tokenizer_model = VECTOR_DB_CONFIG.get('tokenizer_model', 'BAAI/bge-m3')
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_model)
            print(f"âœ… {tokenizer_model} tokenizer ë¡œë“œ ì„±ê³µ")
        except Exception as e:
            print(f"âš ï¸ {tokenizer_model} tokenizer ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("ğŸ’¡ transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ê³  ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•˜ì„¸ìš”.")
            self.tokenizer = None
        
        # TEI ë˜ëŠ” ë¡œì»¬ ëª¨ë¸ ì„ íƒ
        config = VECTOR_DB_CONFIG
        self.use_tei = config.get('tei_enabled', False)
        
        if self.use_tei:
            # TEI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            self.tei_client = TEIClient(
                base_url=config.get('tei_base_url', 'http://localhost:8080'),
                timeout=config.get('tei_timeout', 30)
            )
            
            # TEI ì„œë²„ ì—°ê²° í…ŒìŠ¤íŠ¸
            success, message = self.tei_client.test_connection()
            if success:
                print(f"âœ… {message}")
                print(f"ğŸ“Š TEI ì„œë²„: {config.get('tei_base_url')}")
                print(f"ğŸ¤– ëª¨ë¸: {config.get('tei_model_name', 'BAAI/bge-m3')}")
                print(f"ğŸ“ ì„ë² ë”© ì°¨ì›: {config.get('embedding_dimension', 1024)}")
            else:
                print(f"âŒ {message}")
                print(f"ğŸ’¡ TEI ì„œë²„ë¥¼ ì‹œì‘í•˜ê±°ë‚˜ config.pyì—ì„œ tei_enabled=Falseë¡œ ì„¤ì •í•˜ì„¸ìš”")
                raise RuntimeError(f"TEI ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {message}")
        else:
            # ë¡œì»¬ sentence-transformers ëª¨ë¸ ì‚¬ìš©
            try:
                model_name = config.get('local_embedding_model', 'all-MiniLM-L6-v2')
                self.embedding_model = SentenceTransformer(model_name)
                print(f"âœ… ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ: {model_name}")
            except Exception as e:
                print(f"âš ï¸ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                print("ğŸ’¡ sentence-transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
                raise
    
    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                text = ""
                
                for page_num, page in enumerate(pdf_reader.pages):
                    page_text = page.extract_text()
                    text += f"\n[Page {page_num + 1}]\n{page_text}\n"
                
                return text
        except Exception as e:
            print(f"PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return ""
    
    def clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ì œ"""
        # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        text = re.sub(r'\s+', ' ', text)
        # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
        text = re.sub(r'[^\w\sê°€-í£.,;:()\[\]-]', ' ', text)
        # ë„ˆë¬´ ì§§ì€ ë‹¨ì–´ ì œê±°
        words = text.split()
        words = [word for word in words if len(word) > 1]
        return ' '.join(words)
    
    def chunk_by_tokens(self, text: str, chunk_size: int = None, overlap_ratio: float = None) -> List[str]:
        """BGE-M3 tokenizer ê¸°ë°˜ ì •í™•í•œ token ì²­í‚¹
        
        Args:
            text: ì²­í‚¹í•  í…ìŠ¤íŠ¸
            chunk_size: ì²­í¬ë‹¹ í† í° ìˆ˜ (Noneì´ë©´ config ì‚¬ìš©)
            overlap_ratio: ì˜¤ë²„ë© ë¹„ìœ¨ 0~1 (Noneì´ë©´ config ì‚¬ìš©)
        
        Returns:
            List[str]: ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        if not self.tokenizer:
            # tokenizerê°€ ì—†ìœ¼ë©´ character ê¸°ë°˜ ë°©ì‹ìœ¼ë¡œ fallback
            print("âš ï¸ Tokenizer ë¹„í™œì„±í™”, character ê¸°ë°˜ ì²­í‚¹ ì‚¬ìš©")
            return None
        
        # configì—ì„œ ê¸°ë³¸ê°’ ê°€ì ¸ì˜¤ê¸°
        if chunk_size is None:
            chunk_size = VECTOR_DB_CONFIG.get('chunk_tokens', 512)
        if overlap_ratio is None:
            overlap_ratio = VECTOR_DB_CONFIG.get('overlap_ratio', 0.15)
        
        # overlapì„ í† í° ìˆ˜ë¡œ ê³„ì‚°
        overlap_tokens = int(chunk_size * overlap_ratio)
        
        try:
            # í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë³€í™˜
            tokens = self.tokenizer.encode(text, add_special_tokens=False)
            chunks = []
            
            # ì˜¤ë²„ë©ì„ ê³ ë ¤í•˜ì—¬ ìŠ¬ë¼ì´ë”©
            stride = chunk_size - overlap_tokens
            for i in range(0, len(tokens), stride):
                chunk_tokens = tokens[i:i + chunk_size]
                chunk_text = self.tokenizer.decode(chunk_tokens, skip_special_tokens=True)
                chunks.append(chunk_text)
            
            return chunks
        except Exception as e:
            print(f"âš ï¸ Token ê¸°ë°˜ ì²­í‚¹ ì‹¤íŒ¨: {e}")
            return None
    
    def semantic_chunking(self, text: str) -> List[Dict[str, any]]:
        """ì˜ë¯¸ë¡ ì  ì²­í‚¹ (BGE-M3 tokenizer ê¸°ë°˜)"""
        # BGE-M3 tokenizer ê¸°ë°˜ ì²­í‚¹ ì‹œë„
        if self.tokenizer:
            try:
                # Token ê¸°ë°˜ ì²­í‚¹ (configì—ì„œ ê°€ì ¸ì˜¤ê¸°)
                chunk_texts = self.chunk_by_tokens(text)
                
                if chunk_texts:
                    chunks = []
                    for chunk_id, chunk_text in enumerate(chunk_texts):
                        # í…ìŠ¤íŠ¸ ì •ì œ ë° ë©”íƒ€ë°ì´í„° ì¶”ê°€
                        cleaned_text = self.clean_text(chunk_text)
                        if cleaned_text.strip():  # ë¹ˆ ì²­í¬ ì œì™¸
                            chunks.append({
                                'id': chunk_id,
                                'content': cleaned_text,
                                'length': len(cleaned_text)
                            })
                    
                    chunk_tokens = VECTOR_DB_CONFIG.get('chunk_tokens', 512)
                    overlap_ratio = VECTOR_DB_CONFIG.get('overlap_ratio', 0.15)
                    print(f"âœ… Token ê¸°ë°˜ ì²­í‚¹ ì„±ê³µ: {len(chunks)}ê°œ ì²­í¬ ({chunk_tokens} tokens, {int(overlap_ratio*100)}% overlap)")
                    return chunks
            except Exception as e:
                print(f"âš ï¸ Token ê¸°ë°˜ ì²­í‚¹ ì‹¤íŒ¨, character ê¸°ë°˜ ì²­í‚¹ìœ¼ë¡œ ëŒ€ì²´: {e}")
        
        # Fallback: Character ê¸°ë°˜ ì²­í‚¹ (token configì—ì„œ ê³„ì‚°)
        chunk_tokens = VECTOR_DB_CONFIG.get('chunk_tokens', 512)
        chars_per_token = VECTOR_DB_CONFIG.get('chars_per_token', 4)
        overlap_ratio = VECTOR_DB_CONFIG.get('overlap_ratio', 0.15)
        
        self.chunk_size = chunk_tokens * chars_per_token  # 512 * 4 = 2048
        self.chunk_overlap = int(self.chunk_size * overlap_ratio)  # 2048 * 0.15 = 307
        
        print(f"ğŸ’¡ Character ê¸°ë°˜ fallback: {self.chunk_size}ì ({self.chunk_overlap}ì overlap, {int(overlap_ratio*100)}%)")
        paragraphs = text.split('\n\n')
        chunks = []
        current_chunk = ""
        chunk_id = 0
        
        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if not paragraph:
                continue
            
            # ì²­í¬ í¬ê¸° ì²´í¬
            if len(current_chunk) + len(paragraph) > self.chunk_size:
                if current_chunk:
                    chunks.append({
                        'id': chunk_id,
                        'content': self.clean_text(current_chunk),
                        'length': len(current_chunk)
                    })
                    chunk_id += 1
                
                # ì˜¤ë²„ë© ì²˜ë¦¬
                if self.chunk_overlap > 0 and current_chunk:
                    overlap_text = current_chunk[-self.chunk_overlap:]
                    current_chunk = overlap_text + " " + paragraph
                else:
                    current_chunk = paragraph
            else:
                current_chunk += " " + paragraph
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
        if current_chunk:
            chunks.append({
                'id': chunk_id,
                'content': self.clean_text(current_chunk),
                'length': len(current_chunk)
            })
        
        return chunks
    
    def generate_embeddings(self, chunks: List[Dict]) -> List[Dict]:
        """ì„ë² ë”© ìƒì„±"""
        print("ì„ë² ë”© ìƒì„± ì¤‘...")
        
        contents = [chunk['content'] for chunk in chunks]
        
        # TEI ë˜ëŠ” ë¡œì»¬ ëª¨ë¸ ì‚¬ìš©
        if self.use_tei:
            embeddings = self.tei_client.encode(contents)
        else:
            embeddings = self.embedding_model.encode(contents, show_progress_bar=True)
            embeddings = [emb.tolist() for emb in embeddings]
        
        for i, chunk in enumerate(chunks):
            chunk['embedding'] = embeddings[i] if isinstance(embeddings[i], list) else embeddings[i].tolist()
        
        return chunks
