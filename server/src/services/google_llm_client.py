
import asyncio
import traceback
import google.generativeai as genai
from typing import List, Dict, Any
from .llm_client_interface import LLMClientInterface
from ..core.config import API_KEYS, NODE_EXECUTION_CONFIG

class GoogleLLMClient(LLMClientInterface):
    """Google AI Studio API í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self):
        """Google AI API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        self.api_key = API_KEYS["google"]
        self.client = None
        
        if genai is None:
            return
        
        if self.api_key:
            try:
                genai.configure(api_key=self.api_key)
                self.client = genai
            except Exception:
                self.client = None
    
    def is_available(self) -> bool:
        """Google AI API ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        return genai is not None and self.client is not None and self.api_key is not None
    
    def get_available_models(self) -> List[Dict[str, Any]]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ Google AI ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
        if not self.is_available():
            return []
        
        try:
            # thinking í•„ë“œ ë²„ê·¸ ìš°íšŒ: REST API ì§ì ‘ í˜¸ì¶œ
            import requests
            
            url = "https://generativelanguage.googleapis.com/v1beta/models"
            headers = {"x-goog-api-key": self.api_key}
            
            response = requests.get(url, headers=headers)
            response.raise_for_status()
            
            data = response.json()
            models_data = data.get('models', [])
            
            available_models = []
            for model_data in models_data:
                model_name = model_data.get('name', '').replace('models/', '')
                supported_methods = model_data.get('supportedGenerationMethods', [])
                                
                # ìƒì„± ê°€ëŠ¥í•œ ëª¨ë¸ë§Œ í•„í„°ë§
                if 'generateContent' in supported_methods:
                    model_info = {
                        "value": model_name,
                        "label": model_name,
                        "provider": "google",
                        "model_type": model_name,
                        "disabled": False
                    }
                    available_models.append(model_info)            
            return available_models
            
        except Exception as e:
            return []
    
    async def generate_stream(
        self, 
        prompt: str, 
        model: str, 
        temperature: float = 0.3, 
        max_tokens: int = NODE_EXECUTION_CONFIG["max_tokens_default"]
    ):
        """ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì‘ë‹µ ìƒì„± (í†µí•©ëœ ë‹¨ì¼ ì¸í„°í˜ì´ìŠ¤)"""
        if not self.is_available():
            raise Exception("Google AI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        try:
            # ëª¨ë¸ ì´ë¦„ì— 'models/' ì ‘ë‘ì‚¬ê°€ ì—†ìœ¼ë©´ ì¶”ê°€
            if not model.startswith('models/'):
                model = f'models/{model}'
            
            # ìƒì„±í˜• ëª¨ë¸ ì´ˆê¸°í™” (temperature ë“± ì„¤ì •)
            generation_config = {
                "temperature": temperature,
                "max_output_tokens": max_tokens,
            }
            
            genai_model = genai.GenerativeModel(
                model, 
                generation_config=generation_config
            )
            
            print(f"ğŸ”„ Google AI ì‘ë‹µ ìƒì„± ì‹œì‘...")
            
            # ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± (executorë¥¼ í†µí•œ ì™„ì „ ë³‘ë ¬í™”)
            try:
                import concurrent.futures
                
                # ë™ê¸° ìŠ¤íŠ¸ë¦¬ë°ì„ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰í•˜ì—¬ ë¸”ë¡œí‚¹ ë°©ì§€
                def _sync_generate():
                    try:
                        response = genai_model.generate_content(prompt, stream=True)
                        chunks = []
                        for chunk in response:
                            if hasattr(chunk, 'text') and chunk.text:
                                chunks.append(chunk.text)
                        return chunks
                    except Exception as e:
                        raise e
                
                # ThreadPoolExecutorë¡œ ì™„ì „ ë¹„ë™ê¸°í™”
                loop = asyncio.get_event_loop()
                with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                    chunks = await loop.run_in_executor(executor, _sync_generate)
                
                print(f"âœ… Google AI ì‘ë‹µ ê°ì²´ ìƒì„± ì™„ë£Œ")
                
                # ë¹„ë™ê¸°ì ìœ¼ë¡œ ì²­í¬ ì „ì†¡
                for chunk in chunks:
                    yield chunk
                    await asyncio.sleep(0.01)  # ë‹¤ë¥¸ íƒœìŠ¤í¬ì—ê²Œ ì œì–´ê¶Œ ì–‘ë³´
                        
            except Exception as stream_e:
                error_detail = traceback.format_exc()
                print(f"âš ï¸ Google AI ìŠ¤íŠ¸ë¦¼ ìƒì„± ì˜¤ë¥˜: {stream_e}")
                print(f"âš ï¸ ìƒì„¸ ì˜¤ë¥˜ ì •ë³´:\n{error_detail}")
                raise
                    
        except Exception as e:
            error_msg = str(e)
            # íŠ¹ì • ì—ëŸ¬ ë©”ì‹œì§€ì— ëŒ€í•œ ë” ëª…í™•í•œ ì„¤ëª… ì œê³µ
            if "finish_message" in error_msg:
                error_msg = f"Google AI API êµ¬ì¡° ë³€ê²½ìœ¼ë¡œ ì¸í•œ ì˜¤ë¥˜: {error_msg}"
            elif "Unknown field" in error_msg:
                error_msg = f"Google AI API í•„ë“œ ë³€ê²½ìœ¼ë¡œ ì¸í•œ ì˜¤ë¥˜: {error_msg}"
            
            raise Exception(f"Google AI ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {error_msg}")
        