"""
Nodeë³„ ì‹¤í–‰ì êµ¬í˜„ - ê°„ì†Œí™”ëœ ë²„ì „
í•µì‹¬ ê¸°ëŠ¥ë§Œ ìœ ì§€í•˜ê³  ë¶ˆí•„ìš”í•œ ë³µì¡ì„± ì œê±°
"""

import asyncio
import time
from typing import List, Any, AsyncGenerator

from ..core.models import WorkflowNode, NodeExecutionResult
from ..core.output_parser import ResultParser
from ..core.config import LLM_CONFIG
from ..services.llm_factory import LLMFactory
from ..services.vector_store_service import VectorStoreService


class NodeExecutor:
    """ë…¸ë“œ ì‹¤í–‰ì - ëª¨ë“  ë…¸ë“œ íƒ€ì…ì„ ì²˜ë¦¬"""
    
    def __init__(self):
        self.llm_factory = LLMFactory()
        self.vector_store_service = VectorStoreService()
        self.result_parser = ResultParser()

    async def execute_node(self, node: WorkflowNode, pre_outputs: List[str], rerank_enabled: bool) -> NodeExecutionResult:
        """ë…¸ë“œ ì‹¤í–‰ (ë ˆê±°ì‹œ ì¸í„°í˜ì´ìŠ¤)"""
        return await self.execute_node_with_context(node, pre_outputs, [], rerank_enabled)

    async def execute_node_with_context(self, node: WorkflowNode, pre_outputs: List[str], context_outputs: List[str], rerank_enabled: bool) -> NodeExecutionResult:
        """ë…¸ë“œ ì‹¤í–‰ - context-node ì¶œë ¥ê³¼ ì¼ë°˜ pre-node ì¶œë ¥ì„ ë¶„ë¦¬í•´ì„œ ì²˜ë¦¬"""
        try:
            if self._is_text_node(node.type):
                # Text ë…¸ë“œëŠ” ì¼ë°˜ pre_outputsë§Œ ì‚¬ìš©
                return self._execute_text_node(node, pre_outputs)
            elif node.type == "context-node":
                return await self._execute_context_node(node, pre_outputs, rerank_enabled)
            else:
                # LLM ë…¸ë“œëŠ” contextì™€ input_dataë¥¼ ë¶„ë¦¬í•´ì„œ ì²˜ë¦¬
                return await self._execute_llm_node_with_context(node, pre_outputs, context_outputs, rerank_enabled)
        except Exception as e:
            return NodeExecutionResult(
                node_id=node.id,
                success=False,
                error=f"Node execution failed: {str(e)}",
                execution_time=0.0
            )
    
    async def execute_node_stream(self, node: WorkflowNode, pre_outputs: List[str], rerank_enabled: bool):
        """ë…¸ë“œ ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ (ë ˆê±°ì‹œ ì¸í„°í˜ì´ìŠ¤)"""
        async for chunk in self.execute_node_stream_with_context(node, pre_outputs, [], rerank_enabled):
            yield chunk

    async def execute_node_stream_with_context(self, node: WorkflowNode, pre_outputs: List[str], context_outputs: List[str], rerank_enabled: bool):
        """ë…¸ë“œ ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ - context-node ì¶œë ¥ê³¼ ì¼ë°˜ pre-node ì¶œë ¥ì„ ë¶„ë¦¬í•´ì„œ ì²˜ë¦¬"""
        try:
            if self._is_text_node(node.type):
                # í…ìŠ¤íŠ¸ ë…¸ë“œëŠ” ì¦‰ì‹œ ê²°ê³¼ ë°˜í™˜
                result = self._execute_text_node(node, pre_outputs)
                yield {
                    "type": "result",
                    "success": result.success,
                    "output": result.output,
                    "description": result.description,
                    "execution_time": result.execution_time,
                    "error": result.error
                }
            elif node.type == "context-node":
                # ì»¨í…ìŠ¤íŠ¸ ë…¸ë“œëŠ” ì¦‰ì‹œ ê²°ê³¼ ë°˜í™˜
                result = await self._execute_context_node(node, pre_outputs, rerank_enabled)
                yield {
                    "type": "result", 
                    "success": result.success,
                    "output": result.output,
                    "description": result.description,
                    "execution_time": result.execution_time,
                    "error": result.error
                }
            else:
                # LLM ë…¸ë“œëŠ” contextì™€ input_dataë¥¼ ë¶„ë¦¬í•´ì„œ ì²˜ë¦¬
                async for chunk in self._execute_llm_node_stream_with_context(node, pre_outputs, context_outputs, rerank_enabled):
                    yield chunk
        except Exception as e:
            yield {
                "type": "error",
                "message": f"Node execution failed: {str(e)}"
            }
    
    def _is_text_node(self, node_type: str) -> bool:
        """í…ìŠ¤íŠ¸ ë…¸ë“œ ì—¬ë¶€ í™•ì¸"""
        return node_type in ["input-node", "output-node"]
    
    def _execute_text_node(self, node: WorkflowNode, pre_outputs: List[str]) -> NodeExecutionResult:
        """í…ìŠ¤íŠ¸ ë…¸ë“œ ì‹¤í–‰ (Input/Output)"""
        if node.type == "input-node":
            content = node.content or "ì…ë ¥ ë°ì´í„°ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        else:  # Output node
            if pre_outputs:
                # pre_outputsì—ì„œ <output> íƒœê·¸ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  íŒŒì‹± ì‹œë„
                combined_content = "\n".join(pre_outputs)
                try:
                    # output parsing ì‹œë„
                    parsed_result = self.result_parser.parse_node_output(combined_content)
                    content = parsed_result.output
                except:
                    # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì‚¬ìš©
                    content = combined_content
            else:
                content = node.content or ""

        return NodeExecutionResult(
            node_id=node.id,
            success=True,
            description=content,
            output=content,
            execution_time=0.0
        )
    
    async def _execute_llm_node(self, node: WorkflowNode, pre_outputs: List[str], rerank_enabled: bool) -> NodeExecutionResult:
        """LLM ë…¸ë“œ ì‹¤í–‰ (Generation/Ensemble/Validation)"""
        start_time = time.time()
        
        try:
            prompt = await self._prepare_prompt(node, pre_outputs, rerank_enabled)
            
            if not node.llm_provider or not node.model_type:
                raise ValueError(f"Node {node.id} missing LLM configuration")
            
            llm_client = self.llm_factory.get_client(node.llm_provider)
            response = await self._call_llm(llm_client, node.model_type, prompt)
            
            parsed_result = self.result_parser.parse_node_output(response)
            execution_time = time.time() - start_time
            
            return NodeExecutionResult(
                node_id=node.id,
                success=True,
                description=parsed_result.description,
                output=parsed_result.output,
                execution_time=execution_time
            )
        except Exception as e:
            execution_time = time.time() - start_time
            return NodeExecutionResult(
                node_id=node.id,
                success=False,
                error=f"LLM node execution failed: {str(e)}",
                execution_time=execution_time
            )

    async def _execute_llm_node_with_context(self, node: WorkflowNode, pre_outputs: List[str], context_outputs: List[str], rerank_enabled: bool) -> NodeExecutionResult:
        """LLM ë…¸ë“œ ì‹¤í–‰ - contextì™€ input_dataë¥¼ ë¶„ë¦¬í•´ì„œ ì²˜ë¦¬"""
        start_time = time.time()
        
        try:
            # input_dataëŠ” ì¼ë°˜ pre-node ì¶œë ¥ë“¤ë§Œ ì‚¬ìš©
            input_data = "\n".join(pre_outputs) if pre_outputs else ""
            # contextëŠ” context-node ì¶œë ¥ë“¤ ì‚¬ìš©
            context = "\n".join(context_outputs) if context_outputs else ""
            
            prompt_template = node.prompt or ""
            
            # í”„ë¡¬í”„íŠ¸ ë³€ìˆ˜ ì¹˜í™˜
            formatted_prompt = prompt_template.replace("{input_data}", input_data).replace("{context}", context)
            prompt = formatted_prompt if formatted_prompt.strip() else input_data
            
            if not node.llm_provider or not node.model_type:
                raise ValueError(f"Node {node.id} missing LLM configuration")
            
            llm_client = self.llm_factory.get_client(node.llm_provider)
            response = await self._call_llm(llm_client, node.model_type, prompt)
            
            parsed_result = self.result_parser.parse_node_output(response)
            execution_time = time.time() - start_time
            
            return NodeExecutionResult(
                node_id=node.id,
                success=True,
                description=parsed_result.description,
                output=parsed_result.output,
                execution_time=execution_time
            )
        except Exception as e:
            execution_time = time.time() - start_time
            return NodeExecutionResult(
                node_id=node.id,
                success=False,
                error=f"LLM node execution failed: {str(e)}",
                execution_time=execution_time
            )
    
    async def _execute_llm_node_stream(self, node: WorkflowNode, pre_outputs: List[str], rerank_enabled: bool):
        """LLM ë…¸ë“œ ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰"""
        try:
            # ì§€ì‹ ë² ì´ìŠ¤ ê²€ìƒ‰ ê³¼ì • ìŠ¤íŠ¸ë¦¬ë°
            input_data = "\n".join(pre_outputs) if pre_outputs else ""
            prompt_template = node.prompt or ""
            
            # ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ê³¼ì •ì„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë³´ì—¬ì£¼ê¸°
            context = ""
            if node.knowledge_base and "{context}" in prompt_template:
                import time
                search_start = time.time()
                yield {"type": "stream", "content": f"ğŸ” [{node.id}] ì§€ì‹ ë² ì´ìŠ¤ '{node.knowledge_base}' ê²€ìƒ‰ ì‹œì‘...\n"}
                
                try:
                    rerank_info = None
                    if rerank_enabled and node.llm_provider and node.model_type:
                        rerank_info = {"provider": node.llm_provider, "model": node.model_type}
                        yield {"type": "stream", "content": f"ğŸ”„ [{node.id}] ë¦¬ë­í‚¹ í™œì„±í™”ë¨ ({node.llm_provider}/{node.model_type})\n"}
                    
                    # ê²€ìƒ‰ì„ ë³„ë„ íƒœìŠ¤í¬ë¡œ ì‹¤í–‰í•˜ì—¬ ë³‘ë ¬ ì²˜ë¦¬ ë³´ì¥
                    search_task = asyncio.create_task(
                        self.vector_store_service.search(
                            kb_name=node.knowledge_base,
                            query=input_data,
                            search_intensity=node.search_intensity or "medium",
                            rerank_info=rerank_info
                        )
                    )
                    context_results = await search_task
                    
                    search_time = time.time() - search_start
                    
                    if context_results:
                        context = "\n".join(context_results)
                        yield {"type": "stream", "content": f"âœ… [{node.id}] {len(context_results)}ê°œ ë¬¸ì„œ ì°¾ìŒ ({search_time:.2f}ì´ˆ)\n"}
                    else:
                        context = "No relevant context found."
                        yield {"type": "stream", "content": f"âš ï¸ [{node.id}] ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ ({search_time:.2f}ì´ˆ)\n"}
                        
                except Exception as e:
                    search_time = time.time() - search_start
                    context = f"Context search failed: {str(e)}"
                    yield {"type": "stream", "content": f"âŒ [{node.id}] ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)} ({search_time:.2f}ì´ˆ)\n"}
                    
            elif "{context}" in prompt_template:
                context = "No knowledge base selected."
                yield {"type": "stream", "content": f"âš ï¸ [{node.id}] ì§€ì‹ ë² ì´ìŠ¤ê°€ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n"}
            
            # LLM ì‹¤í–‰ ì‹œì‘ ì•Œë¦¼
            yield {"type": "stream", "content": f"ğŸ¤– [{node.id}] {node.llm_provider}/{node.model_type} ëª¨ë¸ ì‹¤í–‰ ì¤‘...\n\n"}
            
            # í”„ë¡¬í”„íŠ¸ ë³€ìˆ˜ ì¹˜í™˜
            formatted_prompt = prompt_template.replace("{input_data}", input_data).replace("{context}", context)
            prompt = formatted_prompt if formatted_prompt.strip() else input_data

            if not node.llm_provider or not node.model_type:
                raise ValueError(f"Node {node.id} missing LLM configuration")
            
            client = self.llm_factory.get_client(node.llm_provider)
            if not client or not client.is_available():
                raise Exception(f"LLM client not available: {node.llm_provider}")
            
            messages = [{"role": "user", "content": prompt}]
            full_response = ""
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
            if hasattr(client, 'chat_completion_stream'):
                async for chunk in client.chat_completion_stream(
                    model=node.model_type,
                    messages=messages,
                    temperature=LLM_CONFIG["default_temperature"]
                ):
                    if chunk:
                        full_response += chunk
                        yield {"type": "stream", "content": chunk}
            else:
                # ë¹„ìŠ¤íŠ¸ë¦¬ë° í´ë¼ì´ì–¸íŠ¸ì˜ ê²½ìš° ì‹œë®¬ë ˆì´ì…˜
                response = client.chat_completion(
                    model=node.model_type,
                    messages=messages,
                    temperature=LLM_CONFIG["default_temperature"]
                )
                full_response = response
                
                chunk_size = LLM_CONFIG["chunk_processing_size"]
                for i in range(0, len(response), chunk_size):
                    chunk = response[i:i+chunk_size]
                    yield {"type": "stream", "content": chunk}
                    await asyncio.sleep(LLM_CONFIG["simulation_sleep_interval"])
            
            # ê²°ê³¼ íŒŒì‹±
            if full_response:
                parsed_result = self.result_parser.parse_node_output(full_response)
                yield {
                    "type": "parsed_result",
                    "success": True,
                    "description": parsed_result.description,
                    "output": parsed_result.output,
                    "execution_time": 0.0
                }
            else:
                raise Exception("Empty LLM response")
                
        except Exception as e:
            yield {
                "type": "result",
                "success": False,
                "error": str(e),
                "description": f"LLM ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}",
                "output": None
            }
    
    async def _prepare_prompt(self, node: WorkflowNode, pre_outputs: List[str], rerank_enabled: bool) -> str:
        """í”„ë¡¬í”„íŠ¸ ì¤€ë¹„ (ë¹„ìŠ¤íŠ¸ë¦¬ë°)"""
        input_data = "\n".join(pre_outputs) if pre_outputs else ""
        prompt_template = node.prompt or ""
        
        # ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
        context = ""
        if node.knowledge_base and "{context}" in prompt_template:
            try:
                rerank_info = None
                if rerank_enabled and node.llm_provider and node.model_type:
                    rerank_info = {
                        "provider": node.llm_provider,
                        "model": node.model_type
                    }
                
                context_results = await self.vector_store_service.search(
                    kb_name=node.knowledge_base,
                    query=input_data,
                    search_intensity=node.search_intensity or "medium",
                    rerank_info=rerank_info
                )
                context = "\n".join(context_results) if context_results else "No relevant context found."
            except Exception as e:
                context = f"Context search failed: {str(e)}"
        elif "{context}" in prompt_template:
            context = "No knowledge base selected."
        
        # í”„ë¡¬í”„íŠ¸ ë³€ìˆ˜ ì¹˜í™˜
        formatted_prompt = prompt_template.replace("{input_data}", input_data).replace("{context}", context)
        return formatted_prompt if formatted_prompt.strip() else input_data

    async def _execute_context_node(self, node: WorkflowNode, pre_outputs: List[str], rerank_enabled: bool) -> NodeExecutionResult:
        """Context ë…¸ë“œ ì‹¤í–‰ - ë²¡í„° DBì—ì„œ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰"""
        start_time = time.time()
        
        try:
            # ì…ë ¥ ë°ì´í„° ì¤€ë¹„ (pre_outputs ê²°í•©)
            input_data = " ".join(pre_outputs) if pre_outputs else ""
            if not input_data.strip():
                return NodeExecutionResult(
                    node_id=node.id,
                    success=False,
                    error="Context search requires input data from pre-nodes",
                    execution_time=time.time() - start_time
                )
            
            # ì§€ì‹ë² ì´ìŠ¤ ë° ê²€ìƒ‰ ê°•ë„ í™•ì¸
            knowledge_base = node.knowledge_base
            search_intensity = node.search_intensity or "medium"
            
            if not knowledge_base:
                return NodeExecutionResult(
                    node_id=node.id,
                    success=False,
                    error="Knowledge base not specified for context node",
                    execution_time=time.time() - start_time
                )
            
            # ë¦¬ë­í¬ ì„¤ì •
            rerank_info = None
            if rerank_enabled:
                rerank_info = {
                    "provider": LLM_CONFIG.get("default_provider", "google"),
                    "model": LLM_CONFIG.get("default_model", "gemini-2.0-flash")
                }
            
            # ë²¡í„° DB ê²€ìƒ‰ ì‹¤í–‰
            context_results = await self.vector_store_service.search(
                kb_name=knowledge_base,
                query=input_data,
                search_intensity=search_intensity,
                rerank_info=rerank_info
            )
            
            context_content = "\n".join(context_results) if context_results else "No relevant context found."
            
            return NodeExecutionResult(
                node_id=node.id,
                success=True,
                description=f"Found {len(context_results)} relevant context chunks from knowledge base '{knowledge_base}'",
                output=context_content,
                execution_time=time.time() - start_time
            )
            
        except Exception as e:
            return NodeExecutionResult(
                node_id=node.id,
                success=False,
                error=f"Context search failed: {str(e)}",
                execution_time=time.time() - start_time
            )
    
    async def _execute_context_node_stream(self, node: WorkflowNode, pre_outputs: List[str]):
        """Context ë…¸ë“œ ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰"""
        try:
            # ì…ë ¥ ë°ì´í„° ì¤€ë¹„
            query = "\n".join(pre_outputs) if pre_outputs else ""
            
            if not query.strip():
                yield {"type": "result", "success": False, "error": "No input data for context search"}
                return
            
            if not node.knowledge_base:
                yield {"type": "result", "success": False, "error": "No knowledge base selected"}
                return
            
            # ê²€ìƒ‰ ì‹œì‘ ì•Œë¦¼
            yield {"type": "stream", "content": f"ğŸ” [{node.id}] ì§€ì‹ ë² ì´ìŠ¤ '{node.knowledge_base}' ê²€ìƒ‰ ì¤‘...\n"}
            
            # ë²¡í„° ìŠ¤í† ì–´ì—ì„œ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
            context_results = await self.vector_store_service.search(
                kb_name=node.knowledge_base,
                query=query,
                search_intensity=node.search_intensity or "medium",
                rerank_info=None
            )
            
            if context_results:
                context_content = "\n".join(context_results)
                yield {"type": "stream", "content": f"âœ… [{node.id}] {len(context_results)}ê°œì˜ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n"}
                
                yield {
                    "type": "parsed_result",
                    "success": True,
                    "description": f"Found {len(context_results)} relevant context chunks from knowledge base '{node.knowledge_base}'",
                    "output": context_content,
                    "execution_time": 0.0
                }
            else:
                yield {"type": "stream", "content": f"âš ï¸ [{node.id}] ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n"}
                yield {
                    "type": "parsed_result",
                    "success": True,
                    "description": "No relevant context found",
                    "output": "",
                    "execution_time": 0.0
                }
        except Exception as e:
            yield {"type": "result", "success": False, "error": f"Context search failed: {str(e)}"}
    
    async def _call_llm(self, llm_client: Any, model_type: str, prompt: str) -> str:
        """LLM í˜¸ì¶œ"""
        try:
            # generate_responseê°€ async ë©”ì„œë“œì¸ ê²½ìš° ì§ì ‘ í˜¸ì¶œ
            if hasattr(llm_client, 'generate_response'):
                response = await llm_client.generate_response(prompt, model_type)
                return response
            else:
                # ë™ê¸° í´ë¼ì´ì–¸íŠ¸ì˜ ê²½ìš° executor ì‚¬ìš©
                loop = asyncio.get_event_loop()
                response = await loop.run_in_executor(
                    None,
                    lambda: llm_client.generate_response(prompt, model_type)
                )
                return response
        except Exception as e:
            raise Exception(f"LLM call failed: {str(e)}")
    
    async def _execute_llm_node_stream_with_context(self, node: WorkflowNode, pre_outputs: List[str], context_outputs: List[str], rerank_enabled: bool):
        """LLM ë…¸ë“œ ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ - contextì™€ input_dataë¥¼ ë¶„ë¦¬í•´ì„œ ì²˜ë¦¬"""
        try:
            # input_dataëŠ” ì¼ë°˜ pre-node ì¶œë ¥ë“¤ë§Œ ì‚¬ìš©
            input_data = "\n".join(pre_outputs) if pre_outputs else ""
            # contextëŠ” context-node ì¶œë ¥ë“¤ ì‚¬ìš©
            context = "\n".join(context_outputs) if context_outputs else ""
            
            prompt_template = node.prompt or ""
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ ì•Œë¦¼
            yield {"type": "stream", "content": f"ğŸ¤– [{node.id}] Context-aware ì‹¤í–‰: {node.llm_provider}/{node.model_type}\n"}
            
            # í”„ë¡¬í”„íŠ¸ ë³€ìˆ˜ ì¹˜í™˜
            formatted_prompt = prompt_template.replace("{input_data}", input_data).replace("{context}", context)
            prompt = formatted_prompt if formatted_prompt.strip() else input_data

            if not node.llm_provider or not node.model_type:
                raise ValueError(f"Node {node.id} missing LLM configuration")
            
            client = self.llm_factory.get_client(node.llm_provider)
            if not client or not client.is_available():
                raise Exception(f"LLM client not available: {node.llm_provider}")
            
            messages = [{"role": "user", "content": prompt}]
            full_response = ""
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
            if hasattr(client, 'chat_completion_stream'):
                async for chunk in client.chat_completion_stream(
                    model=node.model_type,
                    messages=messages,
                    temperature=LLM_CONFIG["default_temperature"]
                ):
                    if chunk:
                        full_response += chunk
                        yield {"type": "stream", "content": chunk}
            else:
                # ë¹„ìŠ¤íŠ¸ë¦¬ë° í´ë¼ì´ì–¸íŠ¸ì˜ ê²½ìš° ì‹œë®¬ë ˆì´ì…˜
                response = client.chat_completion(
                    model=node.model_type,
                    messages=messages,
                    temperature=LLM_CONFIG["default_temperature"]
                )
                full_response = response
                
                chunk_size = LLM_CONFIG["chunk_processing_size"]
                for i in range(0, len(response), chunk_size):
                    chunk = response[i:i+chunk_size]
                    yield {"type": "stream", "content": chunk}
                    await asyncio.sleep(LLM_CONFIG["simulation_sleep_interval"])
            
            # ê²°ê³¼ íŒŒì‹±
            if full_response:
                parsed_result = self.result_parser.parse_node_output(full_response)
                yield {
                    "type": "parsed_result",
                    "success": True,
                    "description": parsed_result.description,
                    "output": parsed_result.output,
                    "execution_time": 0.0
                }
            else:
                yield {"type": "result", "success": False, "error": "No response received"}
                
        except Exception as e:
            yield {"type": "result", "success": False, "error": str(e)}