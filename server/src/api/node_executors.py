"""
Nodeë³„ ì‹¤í–‰ì êµ¬í˜„ - ê°„ì†Œí™”ëœ ë²„ì „
í•µì‹¬ ê¸°ëŠ¥ë§Œ ìœ ì§€í•˜ê³  ë¶ˆí•„ìš”í•œ ë³µì¡ì„± ì œê±°
"""

import asyncio
import time
from typing import List, Any, AsyncGenerator

from ..core.models import WorkflowNode, NodeExecutionResult, SearchIntensity
from ..core.output_parser import ResultParser
from ..core.config import LLM_CONFIG
from ..services.llm_factory import LLMFactory
from ..services.vector_store_service import VectorStoreService


class NodeExecutor:
    """ë…¸ë“œ ì‹¤í–‰ì - ëª¨ë“  ë…¸ë“œ íƒ€ì…ì„ ì²˜ë¦¬"""
    
    def __init__(self):
        self.llm_factory = LLMFactory()
        self.result_parser = ResultParser()
        # VectorStoreServiceëŠ” í•„ìš”í•  ë•Œë§ˆë‹¤ ìƒˆë¡œ ìƒì„±í•˜ì—¬ ë¸”ë¡œí‚¹ ë°©ì§€

    async def execute_node(self, node: WorkflowNode, pre_outputs: List[str]) -> NodeExecutionResult:
        """ë…¸ë“œ ì‹¤í–‰ (ë ˆê±°ì‹œ ì¸í„°í˜ì´ìŠ¤)"""
        return await self.execute_node_with_context(node, pre_outputs, [])

    async def execute_node_with_context(self, node: WorkflowNode, pre_outputs: List[str], context_outputs: List[str]) -> NodeExecutionResult:
        """ë…¸ë“œ ì‹¤í–‰ - context-node ì¶œë ¥ê³¼ ì¼ë°˜ pre-node ì¶œë ¥ì„ ë¶„ë¦¬í•´ì„œ ì²˜ë¦¬"""
        try:
            if self._is_text_node(node.type):
                # Text ë…¸ë“œëŠ” ì¼ë°˜ pre_outputsë§Œ ì‚¬ìš©
                return self._execute_text_node(node, pre_outputs)
            elif node.type == "context-node":
                return await self._execute_context_node(node, pre_outputs)
            else:
                # LLM ë…¸ë“œëŠ” contextì™€ input_dataë¥¼ ë¶„ë¦¬í•´ì„œ ì²˜ë¦¬
                return await self._execute_llm_node_with_context(node, pre_outputs, context_outputs)
        except Exception as e:
            import traceback
            error_msg = f"Node execution failed: {str(e)}\nTraceback: {traceback.format_exc()}"
            print(f"[NodeExecutor] Error in node {node.id}: {error_msg}")  # Debug log
            return NodeExecutionResult(
                node_id=node.id,
                success=False,
                error=error_msg,
                execution_time=0.0
            )
    
    async def execute_node_stream(self, node: WorkflowNode, pre_outputs: List[str]):
        """ë…¸ë“œ ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ (ë ˆê±°ì‹œ ì¸í„°í˜ì´ìŠ¤)"""
        async for chunk in self.execute_node_stream_with_context(node, pre_outputs, []):
            yield chunk

    async def execute_node_stream_with_context(self, node: WorkflowNode, pre_outputs: List[str], context_outputs: List[str]):
        """ë…¸ë“œ ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ - context-node ì¶œë ¥ê³¼ ì¼ë°˜ pre-node ì¶œë ¥ì„ ë¶„ë¦¬í•´ì„œ ì²˜ë¦¬"""
        try:
            if self._is_text_node(node.type):
                # í…ìŠ¤íŠ¸ ë…¸ë“œëŠ” ì¦‰ì‹œ ê²°ê³¼ ë°˜í™˜
                result = self._execute_text_node(node, pre_outputs)
                yield {
                    "type": "result",
                    "success": result.success,
                    "output": result.output,
                    "description": result.description,
                    "execution_time": result.execution_time,
                    "error": result.error
                }
            elif node.type == "context-node":
                # ì»¨í…ìŠ¤íŠ¸ ë…¸ë“œëŠ” ì¦‰ì‹œ ê²°ê³¼ ë°˜í™˜
                result = await self._execute_context_node(node, pre_outputs)
                yield {
                    "type": "result", 
                    "success": result.success,
                    "output": result.output,
                    "description": result.description,
                    "execution_time": result.execution_time,
                    "error": result.error
                }
            else:
                # LLM ë…¸ë“œëŠ” contextì™€ input_dataë¥¼ ë¶„ë¦¬í•´ì„œ ì²˜ë¦¬
                async for chunk in self._execute_llm_node_stream_with_context(node, pre_outputs, context_outputs):
                    yield chunk
        except Exception as e:
            import traceback
            error_msg = f"Node execution failed: {str(e)}\nTraceback: {traceback.format_exc()}"
            print(f"[NodeExecutor Stream] Error in node {node.id}: {error_msg}")  # Debug log
            yield {
                "type": "error",
                "message": error_msg
            }
    
    def _is_text_node(self, node_type: str) -> bool:
        """í…ìŠ¤íŠ¸ ë…¸ë“œ ì—¬ë¶€ í™•ì¸"""
        return node_type in ["input-node", "output-node"]
    
    def _execute_text_node(self, node: WorkflowNode, pre_outputs: List[str]) -> NodeExecutionResult:
        """í…ìŠ¤íŠ¸ ë…¸ë“œ ì‹¤í–‰ (Input/Output)"""
        if node.type == "input-node":
            content = node.content or "ì…ë ¥ ë°ì´í„°ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        else:  # Output node
            if pre_outputs:
                # pre_outputsì—ì„œ <output> íƒœê·¸ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  íŒŒì‹± ì‹œë„
                combined_content = "\n".join(pre_outputs)
                try:
                    # output parsing ì‹œë„
                    parsed_result = self.result_parser.parse_node_output(combined_content)
                    content = parsed_result.output
                except:
                    # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì‚¬ìš©
                    content = combined_content
            else:
                content = node.content or ""

        return NodeExecutionResult(
            node_id=node.id,
            success=True,
            description=content,
            output=content,
            execution_time=0.0
        )
    
    async def _execute_llm_node(self, node: WorkflowNode, pre_outputs: List[str]) -> NodeExecutionResult:
        """LLM ë…¸ë“œ ì‹¤í–‰ (Generation/Ensemble/Validation)"""
        start_time = time.time()
        
        try:
            prompt = await self._prepare_prompt(node, pre_outputs)
            
            if not node.llm_provider or not node.model_type:
                raise ValueError(f"Node {node.id} missing LLM configuration")
            
            llm_client = self.llm_factory.get_client(node.llm_provider)
            response = await self._call_llm(llm_client, node.model_type, prompt)
            
            parsed_result = self.result_parser.parse_node_output(response)
            execution_time = time.time() - start_time
            
            return NodeExecutionResult(
                node_id=node.id,
                success=True,
                description=parsed_result.description,
                output=parsed_result.output,
                execution_time=execution_time
            )
        except Exception as e:
            execution_time = time.time() - start_time
            return NodeExecutionResult(
                node_id=node.id,
                success=False,
                error=f"LLM node execution failed: {str(e)}",
                execution_time=execution_time
            )

    async def _execute_llm_node_with_context(self, node: WorkflowNode, pre_outputs: List[str], context_outputs: List[str]) -> NodeExecutionResult:
        """LLM ë…¸ë“œ ì‹¤í–‰ - contextì™€ input_dataë¥¼ ë¶„ë¦¬í•´ì„œ ì²˜ë¦¬"""
        start_time = time.time()
        
        try:
            # input_dataëŠ” ì¼ë°˜ pre-node ì¶œë ¥ë“¤ë§Œ ì‚¬ìš©
            input_data = "\n".join(pre_outputs) if pre_outputs else ""
            # contextëŠ” context-node ì¶œë ¥ë“¤ ì‚¬ìš©
            context = "\n".join(context_outputs) if context_outputs else ""
            
            prompt_template = node.prompt or ""
            
            # í”„ë¡¬í”„íŠ¸ ë³€ìˆ˜ ì¹˜í™˜
            formatted_prompt = prompt_template.replace("{input_data}", input_data).replace("{context}", context)
            prompt = formatted_prompt if formatted_prompt.strip() else input_data
            
            if not node.llm_provider or not node.model_type:
                raise ValueError(f"Node {node.id} missing LLM configuration")
            
            llm_client = self.llm_factory.get_client(node.llm_provider)
            response = await self._call_llm(llm_client, node.model_type, prompt)
            
            parsed_result = self.result_parser.parse_node_output(response)
            execution_time = time.time() - start_time
            
            return NodeExecutionResult(
                node_id=node.id,
                success=True,
                description=parsed_result.description,
                output=parsed_result.output,
                execution_time=execution_time
            )
        except Exception as e:
            execution_time = time.time() - start_time
            return NodeExecutionResult(
                node_id=node.id,
                success=False,
                error=f"LLM node execution failed: {str(e)}",
                execution_time=execution_time
            )
    
    async def _execute_llm_node_stream(self, node: WorkflowNode, pre_outputs: List[str]):
        """LLM ë…¸ë“œ ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰"""
        try:
            # LLM ë…¸ë“œì—ì„œëŠ” ì§€ì‹ë² ì´ìŠ¤ ê²€ìƒ‰ì„ í•˜ì§€ ì•ŠìŒ (context-nodeì—ì„œ ì²˜ë¦¬)
            input_data = "\n".join(pre_outputs) if pre_outputs else ""
            prompt_template = node.prompt or ""
            
            # contextëŠ” ë¹ˆ ë¬¸ìì—´ë¡œ ì²˜ë¦¬ (context-nodeì—ì„œ ì œê³µë°›ìŒ)
            context = ""
            
            # LLM ì‹¤í–‰ ì‹œì‘ ì•Œë¦¼
            yield {"type": "stream", "content": f"ğŸ¤– [{node.id}] {node.llm_provider}/{node.model_type} ëª¨ë¸ ì‹¤í–‰ ì¤‘...\n\n"}
            
            # í”„ë¡¬í”„íŠ¸ ë³€ìˆ˜ ì¹˜í™˜
            formatted_prompt = prompt_template.replace("{input_data}", input_data).replace("{context}", context)
            prompt = formatted_prompt if formatted_prompt.strip() else input_data

            if not node.llm_provider or not node.model_type:
                raise ValueError(f"Node {node.id} missing LLM configuration")
            
            client = self.llm_factory.get_client(node.llm_provider)
            if not client or not client.is_available():
                raise Exception(f"LLM client not available: {node.llm_provider}")
            
            full_response = ""
            
            # í†µí•©ëœ ìŠ¤íŠ¸ë¦¬ë° ì¸í„°í˜ì´ìŠ¤ ì‚¬ìš©
            async for chunk in client.generate_stream(
                prompt=prompt,
                model=node.model_type,
                temperature=LLM_CONFIG["default_temperature"]
            ):
                if chunk:
                    full_response += chunk
                    yield {"type": "stream", "content": chunk}
            
            # ê²°ê³¼ íŒŒì‹±
            if full_response:
                parsed_result = self.result_parser.parse_node_output(full_response)
                yield {
                    "type": "parsed_result",
                    "success": True,
                    "description": parsed_result.description,
                    "output": parsed_result.output,
                    "execution_time": 0.0
                }
            else:
                raise Exception("Empty LLM response")
                
        except Exception as e:
            yield {
                "type": "result",
                "success": False,
                "error": str(e),
                "description": f"LLM ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}",
                "output": None
            }
    
    async def _prepare_prompt(self, node: WorkflowNode, pre_outputs: List[str]) -> str:
        """í”„ë¡¬í”„íŠ¸ ì¤€ë¹„ (ë¹„ìŠ¤íŠ¸ë¦¬ë°) - LLM ë…¸ë“œìš©, ì§€ì‹ë² ì´ìŠ¤ ê²€ìƒ‰ ì œê±°"""
        input_data = "\n".join(pre_outputs) if pre_outputs else ""
        prompt_template = node.prompt or ""
        
        # contextëŠ” ë¹ˆ ë¬¸ìì—´ë¡œ ì²˜ë¦¬ (context-nodeì—ì„œ ì œê³µë°›ìŒ)
        context = ""
        
        # í”„ë¡¬í”„íŠ¸ ë³€ìˆ˜ ì¹˜í™˜
        formatted_prompt = prompt_template.replace("{input_data}", input_data).replace("{context}", context)
        return formatted_prompt if formatted_prompt.strip() else input_data

    async def _execute_context_node(self, node: WorkflowNode, pre_outputs: List[str]) -> NodeExecutionResult:
        """
        Context ë…¸ë“œ ì‹¤í–‰ - ë²¡í„° DBì—ì„œ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ + ì‚¬ìš©ì ì •ì˜ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
        ì§€ì‹ë² ì´ìŠ¤ê°€ ì—†ì„ ê²½ìš° additional_contextë§Œ ì‚¬ìš© ê°€ëŠ¥
        """
        start_time = time.time()
        
        try:
            # ì…ë ¥ ë°ì´í„° ì¤€ë¹„ (pre_outputs ê²°í•©)
            input_data = " ".join(pre_outputs) if pre_outputs else ""
            
            # ì§€ì‹ë² ì´ìŠ¤ ë° ê²€ìƒ‰ ê°•ë„ í™•ì¸
            knowledge_base = node.knowledge_base
            search_intensity = node.search_intensity or SearchIntensity.get_default()
            additional_context = node.additional_context or ""
            
            context_parts = []
            total_chunks = 0
            found_chunks = 0
            kb_searched = False  # ì§€ì‹ë² ì´ìŠ¤ ê²€ìƒ‰ ìˆ˜í–‰ ì—¬ë¶€
            
            # ì§€ì‹ë² ì´ìŠ¤ê°€ ì„¤ì •ë˜ì–´ ìˆê³  "none"ì´ ì•„ë‹ˆë©´ ê²€ìƒ‰ ìˆ˜í–‰
            if knowledge_base and knowledge_base.lower() != "none":
                kb_searched = True
                if not input_data.strip():
                    return NodeExecutionResult(
                        node_id=node.id,
                        success=False,
                        error="Context search requires input data from pre-nodes",
                        execution_time=time.time() - start_time
                    )
                
                # context-node ìì²´ì˜ rerank ì„¤ì • ì‚¬ìš©
                rerank_info = None
                if (node.rerank_provider and node.rerank_provider != "none" and node.rerank_model):
                    rerank_info = {
                        "provider": node.rerank_provider,
                        "model": node.rerank_model
                    }
                
                # ë²¡í„° DB ê²€ìƒ‰ ì‹¤í–‰
                vector_store_service = VectorStoreService()
                search_result = await vector_store_service.search(
                    kb_name=knowledge_base,
                    query=input_data,
                    search_intensity=search_intensity,
                    rerank_info=rerank_info
                )
                
                context_results = search_result["chunks"]
                total_chunks = search_result["total_chunks"]
                found_chunks = search_result["found_chunks"]
                
                if context_results:
                    # ì§€ì‹ë² ì´ìŠ¤ ì´ë¦„ë§Œ ì¶œë ¥ ì•ì— ì¶”ê°€ (ì²­í¬ ìˆ˜ ì •ë³´ëŠ” descriptionì— í¬í•¨)
                    kb_header = f"=== Knowledge Base: {knowledge_base} ==="
                    kb_content = "\n".join(context_results)
                    context_parts.append(f"{kb_header}\n{kb_content}")
            
            # ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ì¶”ê°€
            if additional_context.strip():
                context_parts.append(additional_context.strip())
            
            # ìµœì¢… ì»¨í…ìŠ¤íŠ¸ ê²°í•©
            if not context_parts:
                context_content = "No context available."
                if kb_searched:
                    # ê²€ìƒ‰ì€ í–ˆì§€ë§Œ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš°
                    description = f"No context found from KB '{knowledge_base}' ({found_chunks}/{total_chunks} chunks found)"
                else:
                    description = "No knowledge base search performed and no additional context provided."
            else:
                context_content = "\n\n".join(context_parts)
                # descriptionì— ì²­í¬ ìˆ˜ ì •ë³´ í¬í•¨
                if kb_searched:
                    kb_info = f" from KB '{knowledge_base}' ({found_chunks}/{total_chunks} chunks found)"
                else:
                    kb_info = ""
                additional_info = " + user-defined context" if additional_context.strip() else ""
                description = f"Context prepared{kb_info}{additional_info}"
            
            return NodeExecutionResult(
                node_id=node.id,
                success=True,
                description=description,
                output=context_content,
                execution_time=time.time() - start_time
            )
            
        except Exception as e:
            return NodeExecutionResult(
                node_id=node.id,
                success=False,
                error=f"Context preparation failed: {str(e)}",
                execution_time=time.time() - start_time
            )
    
    async def _execute_context_node_stream(self, node: WorkflowNode, pre_outputs: List[str]):
        """
        Context ë…¸ë“œ ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
        ì§€ì‹ë² ì´ìŠ¤ê°€ ì—†ì„ ê²½ìš° additional_contextë§Œ ì‚¬ìš© ê°€ëŠ¥
        """
        try:
            # ì…ë ¥ ë°ì´í„° ì¤€ë¹„
            query = "\n".join(pre_outputs) if pre_outputs else ""
            knowledge_base = node.knowledge_base
            additional_context = node.additional_context or ""
            
            context_parts = []
            total_chunks = 0
            found_chunks = 0
            kb_searched = False  # ì§€ì‹ë² ì´ìŠ¤ ê²€ìƒ‰ ìˆ˜í–‰ ì—¬ë¶€
            
            # ì§€ì‹ë² ì´ìŠ¤ê°€ ì„¤ì •ë˜ì–´ ìˆê³  "none"ì´ ì•„ë‹ˆë©´ ê²€ìƒ‰ ìˆ˜í–‰
            if knowledge_base and knowledge_base.lower() != "none":
                kb_searched = True
                if not query.strip():
                    yield {"type": "result", "success": False, "error": "No input data for context search"}
                    return
                
                # ê²€ìƒ‰ ì‹œì‘ ì•Œë¦¼
                yield {"type": "stream", "content": f"ğŸ” [{node.id}] ì§€ì‹ ë² ì´ìŠ¤ '{knowledge_base}' ê²€ìƒ‰ ì¤‘...\n"}
            
            # rerank ì •ë³´ ì„¤ì •
            rerank_info = None
            if (node.rerank_provider and node.rerank_provider != "none" and node.rerank_model):
                rerank_info = {
                    "provider": node.rerank_provider,
                    "model": node.rerank_model
                }
                yield {"type": "stream", "content": f"ğŸ”„ [{node.id}] ì¬ì •ë ¬ ì„¤ì •ë¨: {node.rerank_provider}/{node.rerank_model}\n"}
            
            # ì§€ì‹ë² ì´ìŠ¤ ê²€ìƒ‰ ìˆ˜í–‰ (ì„¤ì •ë˜ì–´ ìˆê³  "none"ì´ ì•„ë‹ ê²½ìš°)
            if kb_searched:
                # ë²¡í„° ìŠ¤í† ì–´ì—ì„œ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
                vector_store_service = VectorStoreService()
                search_result = await vector_store_service.search(
                    kb_name=knowledge_base,
                    query=query,
                    search_intensity=node.search_intensity or SearchIntensity.get_default(),
                    rerank_info=rerank_info
                )
                
                context_results = search_result["chunks"]
                total_chunks = search_result["total_chunks"]
                found_chunks = search_result["found_chunks"]
                
                if context_results:
                    # ì§€ì‹ë² ì´ìŠ¤ ì´ë¦„ë§Œ ì¶œë ¥ ì•ì— ì¶”ê°€ (ì²­í¬ ìˆ˜ ì •ë³´ëŠ” descriptionê³¼ ìŠ¤íŠ¸ë¦¼ ë©”ì‹œì§€ì— í¬í•¨)
                    kb_header = f"=== Knowledge Base: {knowledge_base} ==="
                    kb_content = "\n".join(context_results)
                    context_parts.append(f"{kb_header}\n{kb_content}")
                    yield {"type": "stream", "content": f"âœ… [{node.id}] ì „ì²´ {total_chunks}ê°œ ì²­í¬ ì¤‘ {found_chunks}ê°œì˜ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n"}
                else:
                    yield {"type": "stream", "content": f"âš ï¸ [{node.id}] ì§€ì‹ë² ì´ìŠ¤ (ì „ì²´ {total_chunks}ê°œ ì²­í¬)ì—ì„œ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n"}
            
            # ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ì¶”ê°€
            if additional_context.strip():
                additional_header = "=== Additional Context ==="
                context_parts.append(f"{additional_header}\n{additional_context.strip()}")
                yield {"type": "stream", "content": f"ğŸ“ [{node.id}] ì‚¬ìš©ì ì •ì˜ ì»¨í…ìŠ¤íŠ¸ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.\n"}
            
            # ìµœì¢… ì»¨í…ìŠ¤íŠ¸ ê²°í•©
            if not context_parts:
                yield {"type": "stream", "content": f"âš ï¸ [{node.id}] ì‚¬ìš© ê°€ëŠ¥í•œ ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.\n"}
                if kb_searched:
                    description = f"No context found from KB '{knowledge_base}' ({found_chunks}/{total_chunks} chunks found)"
                else:
                    description = "No context available"
                yield {
                    "type": "parsed_result",
                    "success": True,
                    "description": description,
                    "output": "No context available.",
                    "execution_time": 0.0
                }
            else:
                context_content = "\n\n".join(context_parts)
                # descriptionì— ì²­í¬ ìˆ˜ ì •ë³´ í¬í•¨
                if kb_searched:
                    kb_info = f" from KB '{knowledge_base}' ({found_chunks}/{total_chunks} chunks)"
                else:
                    kb_info = ""
                additional_info = " + user-defined" if additional_context.strip() else ""
                description = f"Context prepared{kb_info}{additional_info}"
                
                yield {
                    "type": "parsed_result",
                    "success": True,
                    "description": description,
                    "output": context_content,
                    "execution_time": 0.0
                }
        except Exception as e:
            yield {"type": "result", "success": False, "error": f"Context search failed: {str(e)}"}
    
    async def _call_llm(self, llm_client: Any, model_type: str, prompt: str) -> str:
        """LLM í˜¸ì¶œ - ìŠ¤íŠ¸ë¦¬ë° ì¸í„°í˜ì´ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ì²´ ì‘ë‹µ ìˆ˜ì§‘"""
        try:
            # ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì „ì²´ ì‘ë‹µ ìˆ˜ì§‘
            full_response = ""
            async for chunk in llm_client.generate_stream(
                prompt=prompt,
                model=model_type,
                temperature=0.3
            ):
                if chunk:
                    full_response += chunk
            
            if not full_response.strip():
                raise Exception("Empty response from LLM")
                
            return full_response
        except Exception as e:
            raise Exception(f"LLM call failed: {str(e)}")
    
    async def _execute_llm_node_stream_with_context(self, node: WorkflowNode, pre_outputs: List[str], context_outputs: List[str]):
        """LLM ë…¸ë“œ ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ - contextì™€ input_dataë¥¼ ë¶„ë¦¬í•´ì„œ ì²˜ë¦¬"""
        try:
            # input_dataëŠ” ì¼ë°˜ pre-node ì¶œë ¥ë“¤ë§Œ ì‚¬ìš©
            input_data = "\n".join(pre_outputs) if pre_outputs else ""
            # contextëŠ” context-node ì¶œë ¥ë“¤ ì‚¬ìš©
            context = "\n".join(context_outputs) if context_outputs else ""
            
            prompt_template = node.prompt or ""
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ ì•Œë¦¼
            yield {"type": "stream", "content": f"ğŸ¤– [{node.id}] Context-aware ì‹¤í–‰: {node.llm_provider}/{node.model_type}\n"}
            
            # í”„ë¡¬í”„íŠ¸ ë³€ìˆ˜ ì¹˜í™˜
            formatted_prompt = prompt_template.replace("{input_data}", input_data).replace("{context}", context)
            prompt = formatted_prompt if formatted_prompt.strip() else input_data

            if not node.llm_provider or not node.model_type:
                raise ValueError(f"Node {node.id} missing LLM configuration")
            
            client = self.llm_factory.get_client(node.llm_provider)
            if not client or not client.is_available():
                raise Exception(f"LLM client not available: {node.llm_provider}")
            
            full_response = ""
            
            # í†µí•©ëœ ìŠ¤íŠ¸ë¦¬ë° ì¸í„°í˜ì´ìŠ¤ ì‚¬ìš©
            async for chunk in client.generate_stream(
                prompt=prompt,
                model=node.model_type,
                temperature=LLM_CONFIG["default_temperature"]
            ):
                if chunk:
                    full_response += chunk
                    yield {"type": "stream", "content": chunk}
            
            # ê²°ê³¼ íŒŒì‹±
            if full_response:
                parsed_result = self.result_parser.parse_node_output(full_response)
                yield {
                    "type": "parsed_result",
                    "success": True,
                    "description": parsed_result.description,
                    "output": parsed_result.output,
                    "execution_time": 0.0
                }
            else:
                yield {"type": "result", "success": False, "error": "No response received"}
                
        except Exception as e:
            yield {"type": "result", "success": False, "error": str(e)}