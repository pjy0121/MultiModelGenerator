"""
Nodeë³„ ì‹¤í–‰ìž êµ¬í˜„ - ê°„ì†Œí™”ëœ ë²„ì „
í•µì‹¬ ê¸°ëŠ¥ë§Œ ìœ ì§€í•˜ê³  ë¶ˆí•„ìš”í•œ ë³µìž¡ì„± ì œê±°
"""

import asyncio
import time
from typing import List, Any

from ..core.models import WorkflowNode, NodeExecutionResult
from ..core.output_parser import ResultParser
from ..core.config import LLM_CONFIG
from ..services.llm_factory import LLMFactory
from ..services.vector_store_service import VectorStoreService


class NodeExecutor:
    """ë…¸ë“œ ì‹¤í–‰ìž - ëª¨ë“  ë…¸ë“œ íƒ€ìž…ì„ ì²˜ë¦¬"""
    
    def __init__(self):
        self.llm_factory = LLMFactory()
        self.vector_store_service = VectorStoreService()
        self.result_parser = ResultParser()

    async def execute_node(self, node: WorkflowNode, pre_outputs: List[str], rerank_enabled: bool) -> NodeExecutionResult:
        """ë…¸ë“œ ì‹¤í–‰"""
        try:
            if self._is_text_node(node.type):
                return self._execute_text_node(node, pre_outputs)
            else:
                return await self._execute_llm_node(node, pre_outputs, rerank_enabled)
        except Exception as e:
            return NodeExecutionResult(
                node_id=node.id,
                success=False,
                error=f"Node execution failed: {str(e)}",
                execution_time=0.0
            )
    
    async def execute_node_stream(self, node: WorkflowNode, pre_outputs: List[str], rerank_enabled: bool):
        """ë…¸ë“œ ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰"""
        try:
            if self._is_text_node(node.type):
                # í…ìŠ¤íŠ¸ ë…¸ë“œëŠ” ì¦‰ì‹œ ê²°ê³¼ ë°˜í™˜
                result = self._execute_text_node(node, pre_outputs)
                yield {
                    "type": "result",
                    "success": result.success,
                    "output": result.output,
                    "description": result.description,
                    "execution_time": result.execution_time
                }
            else:
                # LLM ë…¸ë“œëŠ” ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
                async for chunk in self._execute_llm_node_stream(node, pre_outputs, rerank_enabled):
                    yield chunk
        except Exception as e:
            yield {
                "type": "error",
                "message": f"Node execution failed: {str(e)}"
            }
    
    def _is_text_node(self, node_type: str) -> bool:
        """í…ìŠ¤íŠ¸ ë…¸ë“œ ì—¬ë¶€ í™•ì¸"""
        return node_type in ["input-node", "output-node"]
    
    def _execute_text_node(self, node: WorkflowNode, pre_outputs: List[str]) -> NodeExecutionResult:
        """í…ìŠ¤íŠ¸ ë…¸ë“œ ì‹¤í–‰ (Input/Output)"""
        if node.type == "input-node":
            content = node.content or "ìž…ë ¥ ë°ì´í„°ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        else:  # Output node
            content = " ".join(pre_outputs) if pre_outputs else (node.content or "")
        
        return NodeExecutionResult(
            node_id=node.id,
            success=True,
            description=content,
            output=content,
            execution_time=0.0
        )
    
    async def _execute_llm_node(self, node: WorkflowNode, pre_outputs: List[str], rerank_enabled: bool) -> NodeExecutionResult:
        """LLM ë…¸ë“œ ì‹¤í–‰ (Generation/Ensemble/Validation)"""
        start_time = time.time()
        
        try:
            prompt = await self._prepare_prompt(node, pre_outputs, rerank_enabled)
            
            if not node.llm_provider or not node.model_type:
                raise ValueError(f"Node {node.id} missing LLM configuration")
            
            llm_client = self.llm_factory.get_client(node.llm_provider)
            response = await self._call_llm(llm_client, node.model_type, prompt)
            
            parsed_result = self.result_parser.parse_node_output(response)
            execution_time = time.time() - start_time
            
            return NodeExecutionResult(
                node_id=node.id,
                success=True,
                description=parsed_result.description,
                output=parsed_result.output,
                execution_time=execution_time
            )
        except Exception as e:
            execution_time = time.time() - start_time
            return NodeExecutionResult(
                node_id=node.id,
                success=False,
                error=f"LLM node execution failed: {str(e)}",
                execution_time=execution_time
            )
    
    async def _execute_llm_node_stream(self, node: WorkflowNode, pre_outputs: List[str], rerank_enabled: bool):
        """LLM ë…¸ë“œ ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰"""
        try:
            # ì§€ì‹ ë² ì´ìŠ¤ ê²€ìƒ‰ ê³¼ì • ìŠ¤íŠ¸ë¦¬ë°
            input_data = "\n".join(pre_outputs) if pre_outputs else ""
            prompt_template = node.prompt or ""
            
            # ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ê³¼ì •ì„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë³´ì—¬ì£¼ê¸°
            context = ""
            if node.knowledge_base and "{context}" in prompt_template:
                import time
                search_start = time.time()
                yield {"type": "stream", "content": f"ðŸ” [{node.id}] ì§€ì‹ ë² ì´ìŠ¤ '{node.knowledge_base}' ê²€ìƒ‰ ì‹œìž‘...\n"}
                
                try:
                    rerank_info = None
                    if rerank_enabled and node.llm_provider and node.model_type:
                        rerank_info = {"provider": node.llm_provider, "model": node.model_type}
                        yield {"type": "stream", "content": f"ðŸ”„ [{node.id}] ë¦¬ëž­í‚¹ í™œì„±í™”ë¨ ({node.llm_provider}/{node.model_type})\n"}
                    
                    # ê²€ìƒ‰ì„ ë³„ë„ íƒœìŠ¤í¬ë¡œ ì‹¤í–‰í•˜ì—¬ ë³‘ë ¬ ì²˜ë¦¬ ë³´ìž¥
                    search_task = asyncio.create_task(
                        self.vector_store_service.search(
                            kb_name=node.knowledge_base,
                            query=input_data,
                            search_intensity=node.search_intensity or "medium",
                            rerank_info=rerank_info
                        )
                    )
                    context_results = await search_task
                    
                    search_time = time.time() - search_start
                    
                    if context_results:
                        context = "\n".join(context_results)
                        yield {"type": "stream", "content": f"âœ… [{node.id}] {len(context_results)}ê°œ ë¬¸ì„œ ì°¾ìŒ ({search_time:.2f}ì´ˆ)\n"}
                    else:
                        context = "No relevant context found."
                        yield {"type": "stream", "content": f"âš ï¸ [{node.id}] ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ ({search_time:.2f}ì´ˆ)\n"}
                        
                except Exception as e:
                    search_time = time.time() - search_start
                    context = f"Context search failed: {str(e)}"
                    yield {"type": "stream", "content": f"âŒ [{node.id}] ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)} ({search_time:.2f}ì´ˆ)\n"}
                    
            elif "{context}" in prompt_template:
                context = "No knowledge base selected."
                yield {"type": "stream", "content": f"âš ï¸ [{node.id}] ì§€ì‹ ë² ì´ìŠ¤ê°€ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n"}
            
            # LLM ì‹¤í–‰ ì‹œìž‘ ì•Œë¦¼
            yield {"type": "stream", "content": f"ðŸ¤– [{node.id}] {node.llm_provider}/{node.model_type} ëª¨ë¸ ì‹¤í–‰ ì¤‘...\n\n"}
            
            # í”„ë¡¬í”„íŠ¸ ë³€ìˆ˜ ì¹˜í™˜
            formatted_prompt = prompt_template.replace("{input_data}", input_data).replace("{context}", context)
            prompt = formatted_prompt if formatted_prompt.strip() else input_data

            if not node.llm_provider or not node.model_type:
                raise ValueError(f"Node {node.id} missing LLM configuration")
            
            client = self.llm_factory.get_client(node.llm_provider)
            if not client or not client.is_available():
                raise Exception(f"LLM client not available: {node.llm_provider}")
            
            messages = [{"role": "user", "content": prompt}]
            full_response = ""
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
            if hasattr(client, 'chat_completion_stream'):
                async for chunk in client.chat_completion_stream(
                    model=node.model_type,
                    messages=messages,
                    temperature=LLM_CONFIG["default_temperature"]
                ):
                    if chunk:
                        full_response += chunk
                        yield {"type": "stream", "content": chunk}
            else:
                # ë¹„ìŠ¤íŠ¸ë¦¬ë° í´ë¼ì´ì–¸íŠ¸ì˜ ê²½ìš° ì‹œë®¬ë ˆì´ì…˜
                response = client.chat_completion(
                    model=node.model_type,
                    messages=messages,
                    temperature=LLM_CONFIG["default_temperature"]
                )
                full_response = response
                
                chunk_size = LLM_CONFIG["chunk_processing_size"]
                for i in range(0, len(response), chunk_size):
                    chunk = response[i:i+chunk_size]
                    yield {"type": "stream", "content": chunk}
                    await asyncio.sleep(LLM_CONFIG["simulation_sleep_interval"])
            
            # ê²°ê³¼ íŒŒì‹±
            if full_response:
                parsed_result = self.result_parser.parse_node_output(full_response)
                yield {
                    "type": "parsed_result",
                    "success": True,
                    "description": parsed_result.description,
                    "output": parsed_result.output,
                    "execution_time": 0.0
                }
            else:
                raise Exception("Empty LLM response")
                
        except Exception as e:
            yield {
                "type": "result",
                "success": False,
                "error": str(e),
                "description": f"LLM ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}",
                "output": None
            }
    
    async def _prepare_prompt(self, node: WorkflowNode, pre_outputs: List[str], rerank_enabled: bool) -> str:
        """í”„ë¡¬í”„íŠ¸ ì¤€ë¹„ (ë¹„ìŠ¤íŠ¸ë¦¬ë°)"""
        input_data = "\n".join(pre_outputs) if pre_outputs else ""
        prompt_template = node.prompt or ""
        
        # ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
        context = ""
        if node.knowledge_base and "{context}" in prompt_template:
            try:
                rerank_info = None
                if rerank_enabled and node.llm_provider and node.model_type:
                    rerank_info = {
                        "provider": node.llm_provider,
                        "model": node.model_type
                    }
                
                context_results = await self.vector_store_service.search(
                    kb_name=node.knowledge_base,
                    query=input_data,
                    search_intensity=node.search_intensity or "medium",
                    rerank_info=rerank_info
                )
                context = "\n".join(context_results) if context_results else "No relevant context found."
            except Exception as e:
                context = f"Context search failed: {str(e)}"
        elif "{context}" in prompt_template:
            context = "No knowledge base selected."
        
        # í”„ë¡¬í”„íŠ¸ ë³€ìˆ˜ ì¹˜í™˜
        formatted_prompt = prompt_template.replace("{input_data}", input_data).replace("{context}", context)
        return formatted_prompt if formatted_prompt.strip() else input_data

    
    async def _call_llm(self, llm_client: Any, model_type: str, prompt: str) -> str:
        """LLM í˜¸ì¶œ"""
        try:
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: llm_client.generate_response(prompt, model_type)
            )
            return response
        except Exception as e:
            raise Exception(f"LLM call failed: {str(e)}")